{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Software Design Using ML&AI nWave\n",
    "\n",
    "\n",
    "# 1. Setup\n",
    "\n",
    "To prepare your environment, you need to install some packages\n",
    "\n",
    "# 1.1 Install the necessary packages\n",
    "\n",
    "You need the latest versions of these packages:<br>\n",
    " \n",
    "** Spacy** a client library for NLP.<br>\n",
    "** Pandas for dataframe.<br>\n",
    "** stop_words: **List of common stop words.<br>\n",
    "** python-boto3:** is a python client for the Boto3 API used for communicating to AWS.<br>\n",
    "** websocket-client: ** is a python client for the Websockets.<br>\n",
    "** pyorient: ** is a python client for the Orient DB.<br><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Install NLTK: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: nltk in /anaconda3/lib/python3.6/site-packages (3.3)\n",
      "Requirement not upgraded as not directly required: six in /anaconda3/lib/python3.6/site-packages (from nltk) (1.11.0)\n",
      "\u001b[33mRetrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\",)': /simple/pyorient/\u001b[0m\n",
      "Requirement already up-to-date: pyorient in /Users/swaroopmishra/.local/lib/python3.6/site-packages (1.5.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade nltk\n",
    "!pip install --upgrade pyorient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install Boto3 client for AWS communication thorugh CLI **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /anaconda3/lib/python3.6/site-packages (1.7.11)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /anaconda3/lib/python3.6/site-packages (from boto3) (0.9.3)\n",
      "Requirement already satisfied: botocore<1.11.0,>=1.10.11 in /Users/swaroopmishra/.local/lib/python3.6/site-packages (from boto3) (1.10.20)\n",
      "Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in /anaconda3/lib/python3.6/site-packages (from boto3) (0.1.13)\n",
      "Requirement already satisfied: docutils>=0.10 in /anaconda3/lib/python3.6/site-packages (from botocore<1.11.0,>=1.10.11->boto3) (0.14)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /anaconda3/lib/python3.6/site-packages (from botocore<1.11.0,>=1.10.11->boto3) (2.6.1)\n",
      "Requirement already satisfied: six>=1.5 in /anaconda3/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.11.0,>=1.10.11->boto3) (1.11.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install boto3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Install stop_words **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stop-words in /anaconda3/lib/python3.6/site-packages (2015.2.23.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install stop-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Install websocket client: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: websocket-client in /anaconda3/lib/python3.6/site-packages (0.47.0)\r\n",
      "Requirement already satisfied: six in /anaconda3/lib/python3.6/site-packages (from websocket-client) (1.11.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install websocket-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Install pyorient: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: awscli in /Users/swaroopmishra/.local/lib/python3.6/site-packages (1.15.20)\n",
      "Requirement already satisfied: docutils>=0.10 in /anaconda3/lib/python3.6/site-packages (from awscli) (0.14)\n",
      "Requirement already satisfied: PyYAML<=3.12,>=3.10 in /anaconda3/lib/python3.6/site-packages (from awscli) (3.12)\n",
      "Requirement already satisfied: colorama<=0.3.9,>=0.2.5 in /anaconda3/lib/python3.6/site-packages (from awscli) (0.3.7)\n",
      "Requirement already satisfied: rsa<=3.5.0,>=3.1.2 in /anaconda3/lib/python3.6/site-packages (from awscli) (3.4.2)\n",
      "Requirement already satisfied: botocore==1.10.20 in /Users/swaroopmishra/.local/lib/python3.6/site-packages (from awscli) (1.10.20)\n",
      "Requirement already satisfied: s3transfer<0.2.0,>=0.1.12 in /anaconda3/lib/python3.6/site-packages (from awscli) (0.1.13)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /anaconda3/lib/python3.6/site-packages (from rsa<=3.5.0,>=3.1.2->awscli) (0.4.2)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /anaconda3/lib/python3.6/site-packages (from botocore==1.10.20->awscli) (0.9.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /anaconda3/lib/python3.6/site-packages (from botocore==1.10.20->awscli) (2.6.1)\n",
      "Requirement already satisfied: six>=1.5 in /anaconda3/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore==1.10.20->awscli) (1.11.0)\n",
      "Requirement already satisfied: pyorient in /Users/swaroopmishra/.local/lib/python3.6/site-packages (1.5.5)\n"
     ]
    }
   ],
   "source": [
    "! pip install awscli\n",
    "! pip install pyorient --user\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Import packages and libraries \n",
    "\n",
    "Import the packages and libraries that you'll use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.cluster.util import cosine_distance\n",
    "from stop_words import get_stop_words\n",
    "import numpy\n",
    "\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "\n",
    "import websocket\n",
    "import _thread\n",
    "import time\n",
    "\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import json\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Configuration\n",
    "\n",
    "Add configurable items of the notebook below\n",
    "## 2.1 Add your service credentials if any required( this is where you need to add credentials of infrastructure you are using to store data etc)\n",
    "\n",
    "\n",
    "Run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This is the section to provide credentials for AWS S3 account\n",
    "### While sharing the notebook remove them -- will try to make this cell hidden later\n",
    "\n",
    "## Console URL :::  https://awstestconsole-swaroop.signin.aws.amazon.com/console\n",
    "## Account Id:Â \n",
    "## Username : \n",
    "## Password : \n",
    "## Then Navigate to the S3 section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Add your service credentials for S3\n",
    "\n",
    "You must create S3 bucket service on AWS. To access data in a file in Object Storage, you need the Object Storage authentication credentials. Insert the Object Storage authentication credentials as credentials_1 in the following cell after removing the current contents in the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @hidden_cell\n",
    "# The following code contains the credentials for a file in your IBM Cloud Object Storage.\n",
    "# You might want to remove those credentials before you share your notebook.\n",
    "credentials_1 = {\n",
    "    'ACCESS_KEY_ID': '',\n",
    "    'ACCESS_SECRET_KEY': '',\n",
    "    'BUCKET': 'software-testing-pyscript'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.  Spacy Text Classification  ( this section will be required if we use spacy for machine learning)\n",
    "\n",
    "Write the classification related utility functions in a modularalized form.\n",
    "\n",
    "## 3.1  REQUIREMENT Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_sentence(text):\n",
    "    \"\"\" Tag the sentence using chunking.\n",
    "    \"\"\"\n",
    "    grammar = \"\"\"\n",
    "      Action: {<VB.?><NN.?>+}\n",
    "      Action: {<VB.?><CLAUSE1><NN.?>+}\n",
    "               }<CLAUSE1>{\n",
    "      Action: {<VB.?><CLAUSE1><CLAUSE1><NN.?>+}\n",
    "               }<CLAUSE1>{\n",
    "      Action: {<VB.?><CLAUSE1><CLAUSE1><CLAUSE1><NN.?>+}\n",
    "               }<CLAUSE1>{  \n",
    "      CLAUSE1: {<DT|PRP.?|IN|JJ>}\n",
    "      \n",
    "      \n",
    "      \"\"\"  \n",
    "    parsed_cp = nltk.RegexpParser(grammar,loop=2)\n",
    "    pos_cp = parsed_cp.parse(text)\n",
    "    return pos_cp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Augumented Classification\n",
    "\n",
    "Custom classification utlity fucntions for augumenting the results of Spacy API call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentences(text):\n",
    "    \"\"\" Split text into sentences.\n",
    "    \"\"\"\n",
    "    sentence_delimiters = re.compile(u'[\\\\[\\\\]\\n.!?]')\n",
    "    sentences = sentence_delimiters.split(text)\n",
    "    return sentences\n",
    "\n",
    "def split_into_tokens(text):\n",
    "    \"\"\" Split text into tokens.\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens\n",
    "    \n",
    "def POS_tagging(text):\n",
    "    \"\"\" Generate Part of speech tagging of the text.\n",
    "    \"\"\"\n",
    "    POSofText = nltk.tag.pos_tag(text)\n",
    "    return POSofText\n",
    "\n",
    "\n",
    "def keyword_tagging(tag,tagtext,text):\n",
    "    \"\"\" Tag the text matching keywords.\n",
    "    \"\"\"\n",
    "    if (text.lower().find(tagtext.lower()) != -1):\n",
    "        return text[text.lower().find(tagtext.lower()):text.lower().find(tagtext.lower())+len(tagtext)]\n",
    "    else:\n",
    "        return 'UNKNOWN'\n",
    "    \n",
    "def regex_tagging(tag,regex,text):\n",
    "    \"\"\" Tag the text matching REGEX.\n",
    "    \"\"\"    \n",
    "    p = re.compile(regex, re.IGNORECASE)\n",
    "    matchtext = p.findall(text)\n",
    "    regex_list=[]    \n",
    "    if (len(matchtext)>0):\n",
    "        for regword in matchtext:\n",
    "            regex_list.append(regword)\n",
    "    return regex_list\n",
    "\n",
    "def BRD_chunk_tagging(tag,chunk,text):\n",
    "    \"\"\" Tag the text using chunking.\n",
    "    \"\"\"\n",
    "    parsed_cp = nltk.RegexpParser(chunk)\n",
    "    pos_cp = parsed_cp.parse(text)\n",
    "    #pos_cp = chunk_sentence(text) #*** use this for getting refined output after chinking but extra entities in output\n",
    "    chunk_list=[]\n",
    "    for root in pos_cp:\n",
    "        if isinstance(root, nltk.tree.Tree):               \n",
    "            if root.label() == tag:\n",
    "                chunk_word = ''\n",
    "                for child_root in root:\n",
    "                    chunk_word = chunk_word +' '+ child_root[0]\n",
    "                chunk_list.append(chunk_word)\n",
    "    return chunk_list\n",
    "\n",
    "def chunk_tagging(tag,chunk,text):\n",
    "    \"\"\" Tag the text using chunking.\n",
    "    \"\"\"\n",
    "    parsed_cp = nltk.RegexpParser(chunk)\n",
    "    pos_cp = parsed_cp.parse(text)\n",
    "    chunk_list=[]\n",
    "    for root in pos_cp:\n",
    "        if isinstance(root, nltk.tree.Tree):               \n",
    "            if root.label() == tag:\n",
    "                chunk_word = ''\n",
    "                for child_root in root:\n",
    "                    chunk_word = chunk_word +' '+ child_root[0]\n",
    "                chunk_list.append(chunk_word)\n",
    "    return chunk_list\n",
    "    \n",
    "def augument_SpResponse(responsejson,updateType,text,tag):\n",
    "    \"\"\" Update the output JSON with augumented classifications.\n",
    "    \"\"\"\n",
    "    if(updateType == 'keyword'):\n",
    "        if not any(d.get('text', None) == text for d in responsejson['Keywords']):\n",
    "            responsejson['Keywords'].append({\"User\":text})\n",
    "    else:\n",
    "        if not any(d.get('text', None) == text for d in responsejson['Entities']):\n",
    "            responsejson['Entities'].append({\"type\":tag,\"text\":text}) \n",
    "\n",
    "def classify_BRD_text(text, config):\n",
    "    \"\"\" Perform augumented classification of the text.\n",
    "    \"\"\"\n",
    "    \n",
    "    #will be used for storing initial value of response json, this is from nlu earlier\n",
    "    with open('output_format_BRD.json') as f:\n",
    "        responsejson = json.load(f)\n",
    "    \n",
    "    sentenceList = split_sentences(text) #\n",
    "    \n",
    "    tokens = split_into_tokens(text)\n",
    "    \n",
    "    postags = POS_tagging(tokens)\n",
    "    \n",
    "    configjson = json.loads(config)#load would take a file-like object, read the data from that object, and use that string to create an object:\n",
    "    \n",
    "    \n",
    "    for stages in configjson['configuration']['classification']['stages']:\n",
    "        # print('Stage - Performing ' + stages['name']+':')\n",
    "        for steps in stages['steps']:\n",
    "            # print('    Step - ' + steps['type']+':')\n",
    "            if (steps['type'] == 'keywords'):\n",
    "                for keyword in steps['keywords']:\n",
    "                        wordtag = tokens[0]\n",
    "                augument_SpResponse(responsejson,'keyword',wordtag,keyword['tag'])\n",
    "            elif(steps['type'] == 'd_regex'):\n",
    "                for regex in steps['d_regex']:\n",
    "                    for word in sentenceList:\n",
    "                        regextags = regex_tagging(regex['tag'],regex['pattern'],word)\n",
    "                        if (len(regextags)>0):\n",
    "                            for words in regextags:\n",
    "                                #print('      '+regex['tag']+':'+words)\n",
    "                                augument_SpResponse(responsejson,'Action',words,regex['tag'])\n",
    "            elif(steps['type'] == 'chunking'):\n",
    "                for chunk in steps['chunk']:\n",
    "                    chunktags = BRD_chunk_tagging(chunk['tag'],chunk['pattern'],postags)\n",
    "                    if (len(chunktags)>0):\n",
    "                        for words in chunktags:\n",
    "                            #print('      '+chunk['tag']+':'+words)\n",
    "                            augument_SpResponse(responsejson,'Action',words,chunk['tag'])\n",
    "            else:\n",
    "                print('UNKNOWN STEP')\n",
    "    \n",
    "    \n",
    "    \n",
    "    return responsejson\n",
    "\n",
    "\n",
    "\n",
    "def classify_text(text, config):\n",
    "    \"\"\" Perform augumented classification of the text.\n",
    "    \"\"\"\n",
    "    \n",
    "    #will be used for storing initial value of response json, this is from nlu earlier\n",
    "    with open('sample.json') as f:\n",
    "        responsejson = json.load(f)\n",
    "    \n",
    "    sentenceList = split_sentences(text) #\n",
    "    \n",
    "    tokens = split_into_tokens(text)\n",
    "    \n",
    "    postags = POS_tagging(tokens)\n",
    "    \n",
    "    configjson = json.loads(config)#load would take a file-like object, read the data from that object, and use that string to create an object:\n",
    "    \n",
    "    for stages in configjson['configuration']['classification']['stages']:\n",
    "        # print('Stage - Performing ' + stages['name']+':')\n",
    "        for steps in stages['steps']:\n",
    "            # print('    Step - ' + steps['type']+':')\n",
    "            if (steps['type'] == 'keywords'):\n",
    "                for keyword in steps['keywords']:\n",
    "                    for word in sentenceList:\n",
    "                        wordtag = keyword_tagging(keyword['tag'],keyword['text'],word)\n",
    "                        if(wordtag != 'UNKNOWN'):\n",
    "                            #print('      '+keyword['tag']+':'+wordtag)\n",
    "                            augument_SpResponse(responsejson,'keyword',wordtag,keyword['tag'])\n",
    "            elif(steps['type'] == 'd_regex'):\n",
    "                for regex in steps['d_regex']:\n",
    "                    for word in sentenceList:\n",
    "                        regextags = regex_tagging(regex['tag'],regex['pattern'],word)\n",
    "                        if (len(regextags)>0):\n",
    "                            for words in regextags:\n",
    "                                #print('      '+regex['tag']+':'+words)\n",
    "                                augument_SpResponse(responsejson,'entities',words,regex['tag'])\n",
    "            elif(steps['type'] == 'chunking'):\n",
    "                for chunk in steps['chunk']:\n",
    "                    chunktags = chunk_tagging(chunk['tag'],chunk['pattern'],postags)\n",
    "                    if (len(chunktags)>0):\n",
    "                        for words in chunktags:\n",
    "                            #print('      '+chunk['tag']+':'+words)\n",
    "                            augument_SpResponse(responsejson,'entities',words,chunk['tag'])\n",
    "            else:\n",
    "                print('UNKNOWN STEP')\n",
    "    \n",
    "    \n",
    "    return responsejson\n",
    "\n",
    "\n",
    "def replace_unicode_strings(response):\n",
    "    \"\"\" Convert dict with unicode strings to strings.\n",
    "    \"\"\"\n",
    "    if isinstance(response, dict):\n",
    "        return {replace_unicode_strings(key): replace_unicode_strings(value) for key, value in response.iteritems()}\n",
    "    elif isinstance(response, list):\n",
    "        return [replace_unicode_strings(element) for element in response]\n",
    "    elif isinstance(response, unicode):\n",
    "        return response.encode('utf-8')\n",
    "    else:\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Correlate text content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = get_stop_words('english')\n",
    "# List of words to be ignored for text similarity\n",
    "stopWords.extend([\"The\",\"This\",\"That\",\".\",\"!\",\"?\"])\n",
    "\n",
    "def compute_text_similarity(text1, text2, text1tags, text2tags):\n",
    "    \"\"\" Compute text similarity using cosine\n",
    "    \"\"\"\n",
    "    #stemming is the process for reducing inflected (or sometimes derived) words to their stem, base or root form\n",
    "    stemmer = nltk.stem.porter.PorterStemmer()\n",
    "    sentences_text1 = split_sentences(text1)\n",
    "    sentences_text2 = split_sentences(text2)\n",
    "    tokens_text1 = []\n",
    "    tokens_text2 = []\n",
    "    \n",
    "    for sentence in sentences_text1:\n",
    "        tokenstemp = split_into_tokens(sentence.lower())\n",
    "        tokens_text1.extend(tokenstemp)\n",
    "    \n",
    "    for sentence in sentences_text2:\n",
    "        tokenstemp = split_into_tokens(sentence.lower())\n",
    "        tokens_text2.extend(tokenstemp)\n",
    "    if (len(text1tags) > 0):  \n",
    "        tokens_text1.extend(text1tags)\n",
    "    if (len(text2tags) > 0):    \n",
    "        tokens_text2.extend(text2tags)\n",
    "    \n",
    "    tokens1Filtered = [stemmer.stem(x) for x in tokens_text1 if x not in stopWords]\n",
    "    \n",
    "    tokens2Filtered = [stemmer.stem(x) for x in tokens_text2 if x not in stopWords]\n",
    "    \n",
    "    #  remove duplicate tokens\n",
    "    tokens1Filtered = set(tokens1Filtered)\n",
    "    tokens2Filtered = set(tokens2Filtered)\n",
    "   \n",
    "    tokensList=[]\n",
    "\n",
    "    text1vector = []\n",
    "    text2vector = []\n",
    "    \n",
    "    if len(tokens1Filtered) < len(tokens2Filtered):\n",
    "        tokensList = tokens1Filtered\n",
    "    else:\n",
    "        tokensList = tokens2Filtered\n",
    "\n",
    "    for token in tokensList:\n",
    "        if token in tokens1Filtered:\n",
    "            text1vector.append(1)\n",
    "        else:\n",
    "            text1vector.append(0)\n",
    "        if token in tokens2Filtered:\n",
    "            text2vector.append(1)\n",
    "        else:\n",
    "            text2vector.append(0)  \n",
    "\n",
    "    cosine_similarity = 1-cosine_distance(text1vector,text2vector)\n",
    "    if numpy.isnan(cosine_similarity):\n",
    "        cosine_similarity = 0\n",
    "    \n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Persistence and Storage\n",
    "## 5.1 Configure Object Storage Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3',\n",
    "                    aws_access_key_id=credentials_1['ACCESS_KEY_ID'],\n",
    "                    aws_secret_access_key=credentials_1['ACCESS_SECRET_KEY'],\n",
    "                    config=Config(signature_version='s3v4')\n",
    "                     )\n",
    "#Enter the path where you want to store data downlaoded from S3\n",
    "\n",
    "\n",
    "def get_file(filename,Location):\n",
    "    #s3.download_file(Bucket=credentials_1['BUCKET'],Key=filename,Filename=Location)\n",
    "    t=\"abc\"\n",
    "\n",
    "#def load_string(fileobject):\n",
    "#    '''Load the file contents into a Python string'''\n",
    "#    text = fileobject.read()\n",
    "#    return text\n",
    "\n",
    "#def load_df(fileobject,sheetname):\n",
    "#    '''Load file contents into a Pandas dataframe'''\n",
    "#    excelFile = pd.ExcelFile(fileobject)\n",
    "#    df = excelFile.parse(sheetname)\n",
    "#    return df\n",
    "\n",
    "#def put_file(filename, filecontents):\n",
    "#    '''Write file to Cloud Object Storage'''\n",
    "#    resp = s3.put_object(Bucket=credentials_1['BUCKET'], Key=filename, Body=filecontents)\n",
    "    #resp = s3.Bucket(Bucket=credentials_1['BUCKET']).put_object(Key=filename, Body=filecontents)\n",
    "#    return resp\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 OrientDB client - functions to connect, store and retrieve data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Connect to OrientDB **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "PyOrientConnectionException",
     "evalue": "Socket Error: [Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyorient/orient.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_socket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mSOCK_CONN_TIMEOUT\u001b[0m \u001b[0;34m)\u001b[0m  \u001b[0;31m# 30 secs of timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_socket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0m_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_socket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mFIELD_SHORT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bytes'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPyOrientConnectionException\u001b[0m               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-6b1356ba33cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0muser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"root\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpassw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"root\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msession_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpassw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyorient/orient.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self, user, password, client_id)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         '''\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ConnectMessage\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpassword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_serialization_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyorient/orient.py\u001b[0m in \u001b[0;36mget_message\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    538\u001b[0m                     \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_auth_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m                 \u001b[0mmessage_instance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m                     \u001b[0;34m.\u001b[0m\u001b[0mset_session_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m                 \u001b[0mmessage_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_push_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_push_received\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyorient/messages/connection.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, _orient_socket)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_orient_socket\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mConnectMessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_orient_socket\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_user\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyorient/messages/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sock)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOrientSocket\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \"\"\"\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_orientSocket\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_protocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_orientSocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyorient/orient.py\u001b[0m in \u001b[0;36mget_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnected\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_socket\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyorient/orient.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPyOrientConnectionException\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"Socket Error: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPyOrientConnectionException\u001b[0m: Socket Error: [Errno 61] Connection refused"
     ]
    }
   ],
   "source": [
    "import pyorient\n",
    "client = pyorient.OrientDB(host=\"localhost\", port=2424)\n",
    "user = \"root\"\n",
    "passw = \"root\"\n",
    "session_id = client.connect(user, passw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** OrientDB Core functions **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_database(dbname, username, password):\n",
    "    \"\"\" Create a database\n",
    "    \"\"\"\n",
    "    client.db_create( dbname, pyorient.DB_TYPE_GRAPH, pyorient.STORAGE_TYPE_MEMORY )\n",
    "    print(dbname  + \" created and opened successfully\")\n",
    "        \n",
    "def drop_database(dbname):\n",
    "    \"\"\" Drop a database\n",
    "    \"\"\"\n",
    "    if client.db_exists( dbname, pyorient.STORAGE_TYPE_MEMORY ):\n",
    "        client.db_drop(dbname)\n",
    "    \n",
    "def create_class(classname):\n",
    "    \"\"\" Create a class\n",
    "    \"\"\"\n",
    "    command = \"create class \"+classname + \" extends V\"\n",
    "    client.command(command)\n",
    "    \n",
    "def create_record(classname, entityname, attributes):\n",
    "    \"\"\" Create a record\n",
    "    \"\"\"\n",
    "    command = \"insert into \" + classname + \" set \" \n",
    "    attrstring = \"\"\n",
    "    for index,key in enumerate(attributes):\n",
    "        attrstring = attrstring + key + \" = '\"+ attributes[key] + \"'\"\n",
    "        if index != len(attributes) -1:\n",
    "            attrstring = attrstring +\",\"\n",
    "    command = command + attrstring\n",
    "    client.command(command)\n",
    "    \n",
    "def create_domain_dataelements_edge(domainid, dataelementid, attributes):\n",
    "    \"\"\" Create an edge between a domain n dataelement \n",
    "    \"\"\"\n",
    "    command = \"create edge linkeddataelements from (select from Domain where ID = \" + \"'\" + domainid + \"') to (select from DataElements where ID = \" + \"'\" + dataelementid + \"')\" \n",
    "    if len(attributes) > 0:\n",
    "        command = command + \" set \"\n",
    "    attrstring = \"\"\n",
    "    for index,key in enumerate(attributes):\n",
    "        val = attributes[key]\n",
    "        if not isinstance(val, str):\n",
    "            val = str(val)\n",
    "        attrstring = attrstring + key + \" = '\"+ val + \"'\"\n",
    "        if index != len(attributes) -1:\n",
    "            attrstring = attrstring +\",\"\n",
    "    command = command + attrstring\n",
    "    print(command)\n",
    "    client.command(command)    \n",
    "    \n",
    "def create_dataelements_requirement_edge(testcaseid, reqid, attributes):\n",
    "    \"\"\" Create an edge between a testcase and a requirement\n",
    "    \"\"\"\n",
    "    command = \"create edge linkedrequirements from (select from DataElements where ID = \"+ \"'\" + testcaseid+\"') to (select from Requirements where ID = \"+\"'\"+reqid+\"')\" \n",
    "    if len(attributes) > 0:\n",
    "        command = command + \" set \"\n",
    "    attrstring = \"\"\n",
    "    for index,key in enumerate(attributes):\n",
    "        val = attributes[key]\n",
    "        if not isinstance(val, str):\n",
    "            val = str(val)\n",
    "        attrstring = attrstring + key + \" = '\"+ val + \"'\"\n",
    "        if index != len(attributes) -1:\n",
    "            attrstring = attrstring +\",\"\n",
    "    command = command + attrstring\n",
    "    client.command(command)  \n",
    "\n",
    "    \n",
    "def create_requirement_domain_edge(reqid, functionalityid, attributes):\n",
    "    \"\"\" Create an edge between a requirement and a domain\n",
    "    \"\"\"\n",
    "    command = \"create edge linkeddomains from (select from Requirements where ID = \"+ \"'\" + reqid+\"') to (select from Domain where ID = \"+\"'\"+functionalityid+\"')\" \n",
    "    \n",
    "    if len(attributes) > 0:\n",
    "         command = command + \" set \"\n",
    "    attrstring = \"\"\n",
    "    for index,key in enumerate(attributes):\n",
    "        val = attributes[key]\n",
    "        if not isinstance(val, str):\n",
    "            val = str(val)\n",
    "        attrstring = attrstring + key + \" = '\"+ val + \"'\"\n",
    "        if index != len(attributes) -1:\n",
    "            attrstring = attrstring +\",\"\n",
    "    command = command + attrstring\n",
    "    print(command)\n",
    "    client.command(command) \n",
    "    \n",
    "def execute_query(query):\n",
    "    \"\"\" Execute a query\n",
    "    \"\"\"\n",
    "    return client.query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** OrientDB Insights **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print(int requirements)? (<ipython-input-82-1b8b1dfd856d>, line 25)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-82-1b8b1dfd856d>\"\u001b[0;36m, line \u001b[0;32m25\u001b[0m\n\u001b[0;31m    print requirements\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(int requirements)?\n"
     ]
    }
   ],
   "source": [
    "def get_related_testcases(defectid):\n",
    "    \"\"\" Get the related testcases for a defect\n",
    "    \"\"\"\n",
    "    testcasesQuery = \"select * from ( select expand( out('linkedtestcases')) from Defect where ID = '\" + defectid +\"' )\"\n",
    "    testcases = execute_query(testcasesQuery)\n",
    "    scoresQuery = \"select expand(out_linkedtestcases) from Defect where ID = '\"+defectid+\"'\"\n",
    "    scores = execute_query(scoresQuery)\n",
    "    testcaseList =[]\n",
    "    scoresList= []\n",
    "    for testcase in testcases:\n",
    "        testcaseList.append(testcase.ID)\n",
    "    for score in scores:\n",
    "        scoresList.append(score.score)\n",
    "    result = {}\n",
    "    length = len(testcaseList)\n",
    "    for i in range(0, length):\n",
    "        result[testcaseList[i]] = scoresList[i]\n",
    "    return result\n",
    "\n",
    "def get_related_requirements(testcaseid):\n",
    "    \"\"\" Get the related requirements for a testcase\n",
    "    \"\"\"\n",
    "    requirementsQuery = \"select * from ( select expand( out('linkedrequirements') ) from Testcase where ID = '\" + testcaseid +\"' )\"\n",
    "    requirements = execute_query(requirementsQuery)\n",
    "    print requirements\n",
    "    scoresQuery = \"select expand(out_linkedrequirements) from Testcase where ID = '\"+testcaseid+\"'\"\n",
    "    scores = execute_query(scoresQuery)\n",
    "    requirementsList =[]\n",
    "    scoresList= []\n",
    "    for requirement in requirements:\n",
    "        requirementsList.append(requirement.ID)\n",
    "    for score in scores:\n",
    "        scoresList.append(score.score)\n",
    "    result = {}\n",
    "    length = len(requirementsList)\n",
    "    print requirementsList, scoresList\n",
    "    for i in range(0, length):\n",
    "        result[requirementsList[i]] = scoresList[i]\n",
    "    return result\n",
    "\n",
    "def get_related_defects(reqid):\n",
    "    \"\"\" Get the related defects for a requirement\n",
    "    \"\"\"\n",
    "    defectsQuery = \"select * from ( select expand( out('linkeddefects')) from Requirement where ID = '\" + reqid +\"' )\"\n",
    "    defects = execute_query(defectsQuery)\n",
    "    scoresQuery = \"select expand(out_linkeddefects) from Requirement where ID = '\"+reqid+\"'\"\n",
    "    scores = execute_query(scoresQuery)\n",
    "    defectsList =[]\n",
    "    scoresList= []\n",
    "    for defect in defects:\n",
    "        defectsList.append(defect.ID)\n",
    "    for score in scores:\n",
    "        scoresList.append(score.score)\n",
    "    result = {}\n",
    "    length = len(defectsList)\n",
    "    for i in range(0, length):\n",
    "        result[defectsList[i]] = scoresList[i]\n",
    "    return result\n",
    "\n",
    "def build_format_defects_list(defectsResult):\n",
    "    \"\"\" Build and format the OrientDB query results for defects\n",
    "    \"\"\"\n",
    "    defects = []\n",
    "    for defect in defectsResult:\n",
    "        detail = {}\n",
    "        detail['ID'] = defect.ID\n",
    "        detail['Severity'] = defect.Severity\n",
    "        detail['Description'] = defect.Description\n",
    "        defects.append(detail)\n",
    "    return defects\n",
    "\n",
    "def build_format_testcases_list(testcasesResult):\n",
    "    \"\"\" Build and format the OrientDB query results for testcases\n",
    "    \"\"\"\n",
    "    testcases = []\n",
    "    for testcase in testcasesResult:\n",
    "        detail = {}\n",
    "        detail['ID'] = testcase.ID\n",
    "        detail['Category'] = testcase.Category\n",
    "        detail['Description'] = testcase.Description\n",
    "        testcases.append(detail)\n",
    "    return testcases  \n",
    "\n",
    "def build_format_requirements_list(requirementsResult):\n",
    "    \"\"\" Build and format the OrientDB query results for requirements\n",
    "    \"\"\"\n",
    "    requirements = []\n",
    "    for requirement in requirementsResult:\n",
    "        detail = {}\n",
    "        detail['ID'] =requirement.ID\n",
    "        detail['Description'] = requirement.Description\n",
    "        detail['Priority'] = requirement.Priority\n",
    "        requirements.append(detail)\n",
    "    return requirements  \n",
    "\n",
    "def get_defects():\n",
    "    \"\"\" Get all defects\n",
    "    \"\"\"\n",
    "    defectsQuery = \"select * from Defect\"\n",
    "    defectsResult = execute_query(defectsQuery)\n",
    "    defects = build_format_defects_list(defectsResult)\n",
    "    return defects\n",
    "\n",
    "def get_testcases():\n",
    "    \"\"\" Get all testcases\n",
    "    \"\"\"\n",
    "    testcasesQuery = \"select * from Testcase\"\n",
    "    testcasesResult = execute_query(testcasesQuery)\n",
    "    testcases = build_format_testcases_list(testcasesResult)\n",
    "    return testcases\n",
    "\n",
    "def get_requirements():\n",
    "    \"\"\" Get all requirements\n",
    "    \"\"\"\n",
    "    requirementsQuery = \"select * from Requirement\"\n",
    "    requirementsResult =  execute_query(requirementsQuery)\n",
    "    requirements = build_format_requirements_list(requirementsResult)\n",
    "    return requirements  \n",
    "\n",
    "def get_defects_severity(severity):\n",
    "    \"\"\" Get defects of a given severity\n",
    "    \"\"\"\n",
    "    query = \"select * from Defect where Severity = \" + str(severity)\n",
    "    queryResult =  execute_query(query)\n",
    "    defects = build_format_defects_list(queryResult)    \n",
    "    return defects\n",
    "\n",
    "def get_testcases_category(category):\n",
    "    \"\"\" Get testcases of a given category\n",
    "    \"\"\"\n",
    "    testcasesQuery = \"select * from Testcase where Category = '\"+str(category)+\"'\"\n",
    "    testcasesResult = execute_query(testcasesQuery)\n",
    "    testcases = build_format_testcases_list(testcasesResult)\n",
    "    return testcases\n",
    "\n",
    "def get_testcases_zero_defects():\n",
    "    \"\"\" Get testcases that did not generate any defects\n",
    "    \"\"\"\n",
    "    testcasesQuery = \"Select * from Testcase where in('linkedtestcases').size() = 0\"\n",
    "    testcasesResult = execute_query(testcasesQuery)\n",
    "    testcases = build_format_testcases_list(testcasesResult)\n",
    "    print testcases\n",
    "    return testcases\n",
    "\n",
    "def get_defects_zero_testcases():\n",
    "    \"\"\" Get defects that have no associated testcases\n",
    "    \"\"\"\n",
    "    query = \"Select * from Defect where out('linkedtestcases').size() = 0\"\n",
    "    queryResult =  execute_query(query)\n",
    "    defects = build_format_defects_list(queryResult)   \n",
    "    print defects\n",
    "    return defects\n",
    "\n",
    "def get_requirements_zero_defect():\n",
    "    \"\"\" Get requirements that have no defects\n",
    "    \"\"\"\n",
    "    query = \"Select * from Requirement where out('linkeddefects').size() = 0\"\n",
    "    requirementsResult =  execute_query(query)\n",
    "    requirements = build_format_requirements_list(requirementsResult)\n",
    "    return requirements  \n",
    "\n",
    "def get_requirements_zero_testcases():\n",
    "    \"\"\" Get requirements that have no associated testcases\n",
    "    \"\"\"\n",
    "    query = \"Select * from Requirement where in('linkedrequirements').size() = 0\"\n",
    "    requirementsResult =  execute_query(query)\n",
    "    requirements = build_format_requirements_list(requirementsResult)\n",
    "    return requirements  \n",
    "\n",
    "def get_requirement_defects(numdefects):\n",
    "    \"\"\" Get requirements that have more than a given number of defects\n",
    "    \"\"\"\n",
    "    query = \"select ID,Description,Priority from Requirement where out('linkeddefects').size() >= \" + str(numdefects)\n",
    "    requirementsResult =  execute_query(query)\n",
    "    requirements = build_format_requirements_list(requirementsResult)\n",
    "    for requirement in requirements:\n",
    "        num = len(get_related_defects(requirement['ID']))\n",
    "        requirement['defectcount'] = num\n",
    "    return requirements  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Global variables and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the excel file with data in S3 Storage\n",
    "BrdFileName = \"Banking-BRD.xlsx\"\n",
    "# Choose or get as an input as to which Domain it belongs to i.e banking, healthcare etc\n",
    "Domain = \"Banking\"\n",
    "\n",
    "# Name of the config files in Object Storage. Rule_brd will be used specifically for parsing requirement document\n",
    "configFileName = \"sample_config.txt\"\n",
    "BRD_configFileName = \"Rule_BRD.txt\"\n",
    "# Config contents\n",
    "config = None;\n",
    "\n",
    "Path = \".//test/\"\n",
    "# Output excell\n",
    "\n",
    "# Requirements dataframe\n",
    "requirements_file_name = \"Requirements.xlsx\"\n",
    "requirements_sheet_name = \"\".join((Domain,\"-Requirements\"))\n",
    "requirements_df = None\n",
    "\n",
    "# Domain/UseCase dataframe\n",
    "domain_file_name = \"Domain.xlsx\"\n",
    "domain_sheet_name = \"\".join((Domain,\"-Domain\"))\n",
    "domain_df = None\n",
    "\n",
    "# DataElements dataframe\n",
    "dataelements_file_name =\"DataElements.xlsx\"\n",
    "dataelements_sheet_name =\"\".join((Domain,\"-Dataelements\"))\n",
    "dataelements_df = None\n",
    "\n",
    "def load_artifacts():\n",
    "    global requirements_df \n",
    "    global domain_df \n",
    "    global dataelements_df \n",
    "    global config\n",
    "    global BRD_config\n",
    "    global Path\n",
    "    Location = \"\".join((Path,requirements_file_name))\n",
    "    get_file(requirements_file_name,Location)\n",
    "    excel = pd.ExcelFile(Location)\n",
    "    requirements_df = excel.parse(requirements_sheet_name)\n",
    "    Location = \"\".join((Path,domain_file_name))\n",
    "    get_file(domain_file_name,Location)\n",
    "    excel = pd.ExcelFile(Location)\n",
    "    domain_df = excel.parse(domain_sheet_name)\n",
    "    Location = \"\".join((Path,dataelements_file_name))\n",
    "    get_file(dataelements_file_name,Location)\n",
    "    excel = pd.ExcelFile(Location)\n",
    "    dataelements_df = excel.parse(dataelements_sheet_name)\n",
    "    rule_text = open(configFileName)\n",
    "    config = rule_text.read()\n",
    "    BRD_rule_text = open(BRD_configFileName)\n",
    "    BRD_config = BRD_rule_text.read()\n",
    "    \n",
    "    \n",
    "\n",
    "def prepare_artifact_dataframes():\n",
    "    \"\"\" Prepare artifact dataframes by creating necessary output columns\n",
    "    \"\"\"\n",
    "    global requirements_df \n",
    "    global domain_df \n",
    "    global dataelements_df \n",
    "    req_cols_len = len(requirements_df.columns)\n",
    "    dom_cols_len = len(domain_df.columns)\n",
    "    dat_cols_len = len(dataelements_df.columns)\n",
    "    requirements_df.insert(req_cols_len, \"ClassifiedText\",\"\")\n",
    "    requirements_df.insert(req_cols_len+1, \"Keywords\",\"\")\n",
    "    requirements_df.insert(req_cols_len+2, \"DomainMatchScore\",\"\")\n",
    "    \n",
    "    domain_df.insert(dom_cols_len, \"ClassifiedText\",\"\")\n",
    "    domain_df.insert(dom_cols_len+1, \"Keywords\",\"\")\n",
    "    domain_df.insert(dom_cols_len+2, \"DataElementsMatchScore\",\"\")\n",
    "\n",
    "    dataelements_df.insert(dat_cols_len, \"ClassifiedText\",\"\")\n",
    "    dataelements_df.insert(dat_cols_len+1, \"Keywords\",\"\")\n",
    "    dataelements_df.insert(dat_cols_len+2, \"RequirementsMatchScore\",\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Utility functions for Engineering Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mod_req_text_classifier_output(artifact_df, BRD_config, output_column_name):\n",
    "    \"\"\" Add text classifier output to the artifact dataframe based on rule defined in config\n",
    "    \"\"\"\n",
    "    for index, row in artifact_df.iterrows():\n",
    "        summary = row[\"I want to <perform some task>\"]\n",
    "        user = row[\"As a <type of user>\"]\n",
    "        user = \"\".join((user,\" want to \"))\n",
    "        summary = \"\".join((user,summary))\n",
    "        print(\"--------------\")\n",
    "        print(summary)\n",
    "        classifier_journey_output = classify_BRD_text(summary, BRD_config)\n",
    "        print(classifier_journey_output)\n",
    "        artifact_df.set_value(index, output_column_name, classifier_journey_output)\n",
    "    return artifact_df \n",
    "\n",
    "\n",
    "def add_text_classifier_output(artifact_df, config, output_column_name):\n",
    "    \"\"\" Add text classifier output to the artifact dataframe based on rule defined in config\n",
    "    \"\"\"\n",
    "    for index, row in artifact_df.iterrows():\n",
    "        summary = row[\"Description\"]\n",
    "        print(\"--------------\")\n",
    "        print(summary)\n",
    "        classifier_journey_output = classify_text(summary, config)\n",
    "        #print(classifier_journey_output)\n",
    "        artifact_df.set_value(index, output_column_name, classifier_journey_output)\n",
    "    return artifact_df \n",
    "           \n",
    "def add_keywords_entities(artifact_df, classify_text_column_name, output_column_name):\n",
    "    \"\"\" Add keywords and entities to the artifact dataframe\"\"\"\n",
    "    for index, artifact in artifact_df.iterrows():\n",
    "        keywords_array = []\n",
    "        for row in artifact[classify_text_column_name]['Keywords']:\n",
    "            #print(row)\n",
    "            if not row['User'] in keywords_array:\n",
    "                keywords_array.append(row['User'])\n",
    "                \n",
    "        for entities in artifact[classify_text_column_name]['Entities']:\n",
    "            if not entities['text'] in keywords_array:\n",
    "                keywords_array.append(entities['text'])\n",
    "            if not entities['type'] in keywords_array:\n",
    "                keywords_array.append(entities['type'])\n",
    "        artifact_df.set_value(index, output_column_name, keywords_array)\n",
    "        print(keywords_array)\n",
    "    return artifact_df \n",
    "\n",
    "#requirements_df, domain_df, keywords_column_name, output_column_name)\n",
    "\n",
    "def populate_text_similarity_score(artifact_df1, artifact_df2, keywords_column_name, output_column_name):\n",
    "    \"\"\" Populate text similarity score to the artifact dataframes\n",
    "    \"\"\"\n",
    "    heading1 = \"Description\"\n",
    "    heading2 = \"Description\"\n",
    "    \n",
    "    try:\n",
    "        artifact_df1[heading1]\n",
    "    except: \n",
    "        heading1 = \"I want to <perform some task>\"\n",
    "    \n",
    "    try:\n",
    "        artifact_df2[heading2]\n",
    "    except: \n",
    "        heading2 = \"I want to <perform some task>\"    \n",
    "    \n",
    "    \n",
    "    for index1, artifact1 in artifact_df1.iterrows():\n",
    "        matches = []\n",
    "        top_matches = []\n",
    "        for index2, artifact2 in artifact_df2.iterrows():\n",
    "            matches.append({'ID': artifact2['ID'], \n",
    "                            'cosine_score': 0, \n",
    "                            'SubjectID':artifact1['ID']})\n",
    "            cosine_score = compute_text_similarity(\n",
    "                #artifact1['Description'], \n",
    "                #artifact2['Description'], \n",
    "                artifact1[heading1],\n",
    "                artifact2[heading2],\n",
    "                artifact1['Keywords'], \n",
    "                artifact2['Keywords'])\n",
    "            matches[index2][\"cosine_score\"] = cosine_score\n",
    "       \n",
    "        sorted_obj = sorted(matches, key=lambda x : x['cosine_score'], reverse=True)\n",
    "    \n",
    "    # This is where the lower cosine value to be truncated is set and needs to be adjusted based on output\n",
    "    \n",
    "        for obj in sorted_obj:\n",
    "            if obj['cosine_score'] > 0.4:\n",
    "                top_matches.append(obj)\n",
    "               \n",
    "        artifact_df1.set_value(index1, output_column_name, top_matches)\n",
    "    return artifact_df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Process flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Prepare data **\n",
    "* Load artifacts from object storage and create pandas dataframes\n",
    "* Prepare the pandas dataframes. Add additional columns required for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_artifacts()\n",
    "prepare_artifact_dataframes()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Run Text Classification on data **\n",
    "* Add the text classification output to the artifact dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Customer want to deposit cheque in customer name in customer account number\n",
      "{'Keywords': [{'User': ''}, {'User': 'Customer'}], 'Entities': [{'type': '', 'text': ''}, {'type': 'Action', 'text': ' deposit cheque'}]}\n",
      "--------------\n",
      "Customer want to withdraw cash from an ATM\n",
      "{'Keywords': [{'User': ''}, {'User': 'Customer'}], 'Entities': [{'type': '', 'text': ''}, {'type': 'Action', 'text': ' withdraw cash'}]}\n",
      "--------------\n",
      "Customer want to want to transfer money from one account to another\n",
      "{'Keywords': [{'User': ''}, {'User': 'Customer'}], 'Entities': [{'type': '', 'text': ''}, {'type': 'Action', 'text': ' transfer money'}]}\n",
      "--------------\n",
      "Customer want to pay my utility bills online\n",
      "{'Keywords': [{'User': ''}, {'User': 'Customer'}], 'Entities': [{'type': '', 'text': ''}, {'type': 'Action', 'text': ' pay my utility bills'}]}\n",
      "--------------\n",
      "Customer want to apply for a loan\n",
      "{'Keywords': [{'User': ''}, {'User': 'Customer'}], 'Entities': [{'type': '', 'text': ''}, {'type': 'Action', 'text': ' apply for a loan'}]}\n",
      "--------------\n",
      "Banker want to request for check books\n",
      "{'Keywords': [{'User': ''}, {'User': 'Banker'}], 'Entities': [{'type': '', 'text': ''}, {'type': 'Action', 'text': ' request for check books'}]}\n",
      "--------------\n",
      "Banker want to restock sufficient cash in ATM machines\n",
      "{'Keywords': [{'User': ''}, {'User': 'Banker'}], 'Entities': [{'type': '', 'text': ''}, {'type': 'Action', 'text': ' restock sufficient cash'}]}\n",
      "--------------\n",
      "Banker want to limit the cash withdrawl from ATM machines\n",
      "{'Keywords': [{'User': ''}, {'User': 'Banker'}], 'Entities': [{'type': '', 'text': ''}, {'type': 'Action', 'text': ' limit the cash withdrawl'}]}\n",
      "--------------\n",
      "Banker want to I want to review all transactions of the day\n",
      "{'Keywords': [{'User': ''}, {'User': 'Banker'}], 'Entities': [{'type': '', 'text': ''}, {'type': 'Action', 'text': ' review all transactions'}]}\n",
      "--------------\n",
      "Banker want to review the credit history and the bank balance of customers that apply for loans\n",
      "{'Keywords': [{'User': ''}, {'User': 'Banker'}], 'Entities': [{'type': '', 'text': ''}, {'type': 'Action', 'text': ' review the credit history'}, {'type': 'Action', 'text': ' apply for loans'}]}\n",
      "--------------\n",
      "Manager want to operate lockers of customer\n",
      "{'Keywords': [{'User': ''}, {'User': 'Manager'}], 'Entities': [{'type': '', 'text': ''}, {'type': 'Action', 'text': ' operate lockers'}]}\n",
      "--------------\n",
      "Function deals with Verification of PIN entered by Customer at ATM. It will be used for withdrawing money, checking balance  or depositing cheque. Input required for this are Account number, Name of customer \n",
      "--------------\n",
      "This use case deals with checking the current account balance before transferingmoney to other account  or withdrawing any amount. Input required for function is account number and account name\n",
      "--------------\n",
      "The use case deals with the sending any mails or blank cheque books to customer. It will require Account number, Name of Customer and Adddress \n",
      "--------------\n",
      "This use case is intended for transfering money from one account to another. This will require source account number, target account number, amount to br transferred\n",
      "--------------\n",
      "function dealswith deposit cheque at ATM or branch office. Input required for this are Account number, account type and Customer Name\n",
      "--------------\n",
      "This function manages actual process of withdrawing cash, based on bill type and maximum amount limit that can be withdrawn from the ATM.\n",
      "--------------\n",
      "This function is required if a customer is willing to apply for loan. This requires type of loan like car loan, house loan. Also account number is required along with loan amount requested. \n",
      "--------------\n",
      "Function will be used by banker to fill the cash at ATM with required  bill types and necessary amount of cash. \n",
      "--------------\n",
      "Function deals with checking credit history of customer with account number and applying for loan.  The data is collected from external credit rating agency and  credit score of customer account.\n",
      "--------------\n",
      "Customer Name  and will be used for verification, deposit , withdrawal, transfer of money and for all activities \n",
      "--------------\n",
      "Account number of the customer will be used to deposit,withdrawal, verififcation, and other banking activities\n",
      "--------------\n",
      "This number signifies how much money is available in the account\n",
      "--------------\n",
      "Debit Card Number will be required as input for withdrawing amount from ATM\n",
      "--------------\n",
      "From Account Number will be required for transfering money from source account\n",
      "--------------\n",
      "To Account Number will be required for transfering money to destination account\n",
      "--------------\n",
      "this specifies how much amount is to be deposited\n",
      "--------------\n",
      "Account Type will specify which type of account it is and will be used for deposit,  withddrawl, etc\n",
      "--------------\n",
      "This element provides the maximum limit of cash that can be withdrawn from ATM\n",
      "--------------\n",
      "Amount transfer denotes the amount to be transferred from one account to another account\n",
      "--------------\n",
      "This  denotes the amount to be withdrawn from the account\n",
      "--------------\n",
      "This provides the addresss for any mail communication with customer\n",
      "--------------\n",
      "How much loan amount customer is asking. For applying loan this is must have\n",
      "--------------\n",
      "This signifies the purpose of taking loan like cbuying new car or house\n",
      "--------------\n",
      "For approving loan one has to check the credit score which is obtained from external agencies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:13: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  del sys.path[0]\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:26: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    }
   ],
   "source": [
    "output_column_name = \"ClassifiedText\"\n",
    "requirements_df = mod_req_text_classifier_output(requirements_df, BRD_config, output_column_name)\n",
    "\n",
    "domain_df = add_text_classifier_output(domain_df,config, output_column_name)\n",
    "dataelements_df = add_text_classifier_output(dataelements_df,config, output_column_name)\n",
    "\n",
    "#requirements_df.head()\n",
    "#domain_df.head()\n",
    "#dataelements_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Populate keywords and entities **\n",
    "* Add the keywords and entities extracted from the unstructured text to the artifact dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'Customer', ' deposit cheque', 'Action']\n",
      "['', 'Customer', ' withdraw cash', 'Action']\n",
      "['', 'Customer', ' transfer money', 'Action']\n",
      "['', 'Customer', ' pay my utility bills', 'Action']\n",
      "['', 'Customer', ' apply for a loan', 'Action']\n",
      "['', 'Banker', ' request for check books', 'Action']\n",
      "['', 'Banker', ' restock sufficient cash', 'Action']\n",
      "['', 'Banker', ' limit the cash withdrawl', 'Action']\n",
      "['', 'Banker', ' review all transactions', 'Action']\n",
      "['', 'Banker', ' review the credit history', 'Action', ' apply for loans']\n",
      "['', 'Manager', ' operate lockers', 'Action']\n",
      "['', 'Customer', 'customer', ' Function', 'NP', ' money', ' balance', ' cheque', ' number', ' customer', ' Verification', 'NAME', ' PIN', ' Customer', ' ATM', ' Input', ' Account', ' Name', ' be', 'VERB']\n",
      "['', ' This use', 'NP', ' case', ' the current account', ' balance', ' transferingmoney', ' other account', ' any amount', ' function', ' account number', ' account', ' name', ' Input', 'NAME']\n",
      "['', 'customer', 'Customer', ' The use', 'NP', ' case', ' blank cheque', ' customer', ' number', ' Account', 'NAME', ' Name', ' Customer', ' Adddress', ' require', 'VERB']\n",
      "['', ' This use', 'NP', ' case', ' money', ' account', ' source', ' number', ' target', ' amount', ' require', 'VERB', ' br']\n",
      "['', 'Customer', ' function', 'NP', ' deposit', ' cheque', ' branch', ' office', ' Input', ' number', ' type', ' ATM', 'NAME', ' Account', ' Customer Name']\n",
      "['', ' This function', 'NP', ' actual process', ' cash', ' bill', ' type', ' maximum amount', ' limit', ' ATM', 'NAME', ' be', 'VERB']\n",
      "['', 'customer', ' This function', 'NP', ' a customer', ' loan', ' type', ' car', ' house', ' number', ' amount', ' apply', 'VERB']\n",
      "['', 'banker', ' Function', 'NP', ' banker', ' the cash', ' required bill', ' necessary amount', ' cash', ' ATM', 'NAME', ' be', 'VERB', ' fill']\n",
      "['', 'customer', ' Function', 'NP', ' credit', ' history', ' customer', ' account', ' number', ' loan', ' The data', ' external credit', ' rating', ' agency', ' score']\n",
      "['', 'Customer', ' verification', 'NP', ' deposit', ' withdrawal', ' transfer', ' money', ' Customer Name', 'NAME', ' be', 'VERB']\n",
      "['', 'customer', ' number', 'NP', ' the customer', ' withdrawal', ' verififcation', ' other banking', ' Account', 'NAME', ' be', 'VERB', ' deposit']\n",
      "['', ' This number', 'NP', ' much money', ' the account']\n",
      "['', ' input', 'NP', ' amount', ' Debit Card Number', 'NAME', ' ATM', ' be', 'VERB']\n",
      "['', ' money', 'NP', ' source', ' account', ' Account Number', 'NAME', ' be', 'VERB']\n",
      "['', ' money', 'NP', ' destination', ' account', ' Account Number', 'NAME', ' be', 'VERB']\n",
      "['', ' much amount', 'NP', ' be', 'VERB']\n",
      "['', ' type', 'NP', ' account', ' deposit', ' withddrawl', ' Account Type', 'NAME', ' specify', 'VERB', ' be']\n",
      "['', ' This element', 'NP', ' the maximum limit', ' cash', ' ATM', 'NAME', ' be', 'VERB']\n",
      "['', ' transfer', 'NP', ' the amount', ' account', ' another account', ' Amount', 'NAME', ' be', 'VERB']\n",
      "['', ' the amount', 'NP', ' the account', ' be', 'VERB']\n",
      "['', 'customer', ' the addresss', 'NP', ' any mail', ' communication', ' customer']\n",
      "['', 'customer', ' much loan', 'NP', ' amount', ' customer', ' loan', ' have', 'VERB']\n",
      "['', ' the purpose', 'NP', ' loan', ' new car', ' house']\n",
      "['', ' loan', 'NP', ' the credit', ' score', ' check', 'VERB']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:43: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    }
   ],
   "source": [
    "classify_text_column_name = \"ClassifiedText\"\n",
    "output_column_name = \"Keywords\"\n",
    "requirements_df = add_keywords_entities(requirements_df, classify_text_column_name, output_column_name)\n",
    "domain_df = add_keywords_entities(domain_df, classify_text_column_name, output_column_name)\n",
    "dataelements_df = add_keywords_entities(dataelements_df, classify_text_column_name, output_column_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Correlate keywords between artifacts **\n",
    "* Add the text similarity score of associated artifacts to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Short</th>\n",
       "      <th>Description</th>\n",
       "      <th>ClassifiedText</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>RequirementsMatchScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DE1</td>\n",
       "      <td>Cus_Nme</td>\n",
       "      <td>Customer Name  and will be used for verificati...</td>\n",
       "      <td>{'Keywords': [{'User': ''}, {'User': 'Customer...</td>\n",
       "      <td>[, Customer,  verification, NP,  deposit,  wit...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DE2</td>\n",
       "      <td>Acc_num</td>\n",
       "      <td>Account number of the customer will be used to...</td>\n",
       "      <td>{'Keywords': [{'User': ''}, {'User': 'customer...</td>\n",
       "      <td>[, customer,  number, NP,  the customer,  with...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DE3</td>\n",
       "      <td>Amt_avail</td>\n",
       "      <td>This number signifies how much money is availa...</td>\n",
       "      <td>{'Keywords': [{'User': ''}], 'Entities': [{'ty...</td>\n",
       "      <td>[,  This number, NP,  much money,  the account]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DE4</td>\n",
       "      <td>Debit_pin</td>\n",
       "      <td>Debit Card Number will be required as input fo...</td>\n",
       "      <td>{'Keywords': [{'User': ''}], 'Entities': [{'ty...</td>\n",
       "      <td>[,  input, NP,  amount,  Debit Card Number, NA...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DE5</td>\n",
       "      <td>From_AcctNum</td>\n",
       "      <td>From Account Number will be required for trans...</td>\n",
       "      <td>{'Keywords': [{'User': ''}], 'Entities': [{'ty...</td>\n",
       "      <td>[,  money, NP,  source,  account,  Account Num...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID         Short                                        Description  \\\n",
       "0  DE1       Cus_Nme  Customer Name  and will be used for verificati...   \n",
       "1  DE2       Acc_num  Account number of the customer will be used to...   \n",
       "2  DE3     Amt_avail  This number signifies how much money is availa...   \n",
       "3  DE4     Debit_pin  Debit Card Number will be required as input fo...   \n",
       "4  DE5  From_AcctNum  From Account Number will be required for trans...   \n",
       "\n",
       "                                      ClassifiedText  \\\n",
       "0  {'Keywords': [{'User': ''}, {'User': 'Customer...   \n",
       "1  {'Keywords': [{'User': ''}, {'User': 'customer...   \n",
       "2  {'Keywords': [{'User': ''}], 'Entities': [{'ty...   \n",
       "3  {'Keywords': [{'User': ''}], 'Entities': [{'ty...   \n",
       "4  {'Keywords': [{'User': ''}], 'Entities': [{'ty...   \n",
       "\n",
       "                                            Keywords RequirementsMatchScore  \n",
       "0  [, Customer,  verification, NP,  deposit,  wit...                         \n",
       "1  [, customer,  number, NP,  the customer,  with...                         \n",
       "2    [,  This number, NP,  much money,  the account]                         \n",
       "3  [,  input, NP,  amount,  Debit Card Number, NA...                         \n",
       "4  [,  money, NP,  source,  account,  Account Num...                         "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#requirements_df.get_value(0,'ClassifiedText')\n",
    "#domain_df.head()\n",
    "dataelements_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:90: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    }
   ],
   "source": [
    "keywords_column_name = \"Keywords\"\n",
    "output_column_name = \"DomainMatchScore\"\n",
    "requirements_df = populate_text_similarity_score(requirements_df, domain_df, keywords_column_name, output_column_name)\n",
    "\n",
    "output_column_name = \"DataElementsMatchScore\"\n",
    "domain_df = populate_text_similarity_score(domain_df, dataelements_df, keywords_column_name, output_column_name)\n",
    "\n",
    "output_column_name = \"RequirementsMatchScore\"\n",
    "dataelements_df = populate_text_similarity_score(dataelements_df, requirements_df, keywords_column_name, output_column_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This section will be used to create the Output in excell format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_action(summary):\n",
    "    action_string = \"\"\n",
    "    count = 1\n",
    "    for entities in summary['Entities']:\n",
    "        #print(entities['text'])\n",
    "        if not entities['text'] in action_string:\n",
    "                action_string = action_string + entities['text']\n",
    "                count = count + 1\n",
    "                if count == 2:\n",
    "                    count = 1\n",
    "                    action_string = action_string + \",\"\n",
    "    \n",
    "    #print(action_string)\n",
    "    return action_string\n",
    "\n",
    "def lookup_use_case(temp,artifact3_df,column_name):\n",
    "    #print(artifact3_df.get_value(0,'ID'))\n",
    "    val = \"\"\n",
    "    rowNum = len(artifact3_df.index)\n",
    "    #print(rowNum)\n",
    "    for j in range(0,rowNum):\n",
    "        if temp == artifact3_df.get_value(j,'ID'):\n",
    "            val = artifact3_df.get_value(j,column_name)\n",
    "            #print(val)\n",
    "    \n",
    "    return val       \n",
    "        \n",
    "    \n",
    "def extract_match(summary,no_of_matches,artifact3_df,column_name):\n",
    "    match_array_description = []\n",
    "    match_array_id = []\n",
    "    for index in range(0,no_of_matches):\n",
    "        try:\n",
    "            temp = summary[index][\"ID\"]\n",
    "            \n",
    "        except:\n",
    "              break\n",
    "    \n",
    "            \n",
    "        temp = summary[index][\"ID\"]\n",
    "        #print(temp)\n",
    "        use_case = lookup_use_case(temp,artifact3_df,column_name)\n",
    "        \n",
    "        match_array_id.append(temp)\n",
    "        match_array_description.append(use_case + \"(\" + str(round(summary[index][\"cosine_score\"], 2)) +\")\")\n",
    "        #print(use_case)\n",
    "            \n",
    "    \n",
    "    #print(\"************\")\n",
    "    #print(match_array_id)       \n",
    "    #print(\"************\")\n",
    "    return (match_array_description,match_array_id)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "def extract_action_requirements_df(artifact1_df, artifact2_df):\n",
    "    \"\"\" Add text classifier output to the artifact dataframe based on rule defined in config\n",
    "    \"\"\"\n",
    "    for index, row in artifact2_df.iterrows():\n",
    "        summary = row[\"ClassifiedText\"]\n",
    "        classifier_journey_output = extract_action(summary)\n",
    "        artifact1_df.set_value(index, 'Action', classifier_journey_output)\n",
    "    return artifact1_df \n",
    "\n",
    "def extract_bestmatch(artifact1_df, artifact2_df, artifact3_df, artifact4_df):\n",
    "    \"\"\" Add text classifier output to the artifact dataframe based on rule defined in config\n",
    "    \"\"\"\n",
    "    No_of_matches_user_function = 2\n",
    "    No_of_matches_data_elements = 8\n",
    "    best_match_output_domain_function = []\n",
    "    best_match_output_dataelement_function = []\n",
    "    \n",
    "    for index, row in artifact2_df.iterrows():\n",
    "        summary = row[\"DomainMatchScore\"]\n",
    "        #print(summary)\n",
    "        (best_match_output_domain_function,best_match_output_domain_id) = extract_match(summary, No_of_matches_user_function, artifact3_df,\"User Function\")\n",
    "        #print(best_match_output_domain_id)\n",
    "        artifact1_df.set_value(index, 'Functionality Match', best_match_output_domain_function)\n",
    "        \n",
    "        for index2 in best_match_output_domain_id:\n",
    "            #print(index2)\n",
    "            row_domain = len(artifact3_df.index)\n",
    "            for p in range(0,row_domain):\n",
    "                if index2 == artifact3_df.get_value(p,'ID'):\n",
    "                    dataelement_summary = artifact3_df.get_value(p,'DataElementsMatchScore')\n",
    "                    #print(dataelement_summary)\n",
    "                    #print(\"------\")\n",
    "                    (best_match_output_dataelement_function,best_match_output_dataelement_id) = extract_match(dataelement_summary, No_of_matches_data_elements, artifact4_df, \"Short\")\n",
    "            best_match_output_dataelement_function.extend(best_match_output_dataelement_function)\n",
    "          \n",
    "        #print(best_match_output_dataelement_function)\n",
    "        #print(\"==============\")\n",
    "        #print(index)\n",
    "        best_match_output_dataelement_function = list(set(best_match_output_dataelement_function))\n",
    "        artifact1_df.set_value(index, 'Attributes Match', best_match_output_dataelement_function)\n",
    "    return artifact1_df \n",
    "\n",
    "\n",
    "#if not any(d.get('text', None) == text for d in responsejson['Keywords']):\n",
    "#            responsejson['Keywords'].append({\"User\":text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        User                                        Action  \\\n",
      "0   Customer                               deposit cheque,   \n",
      "1   Customer                                withdraw cash,   \n",
      "2   Customer                               transfer money,   \n",
      "3   Customer                         pay my utility bills,   \n",
      "4   Customer                             apply for a loan,   \n",
      "5     Banker                      request for check books,   \n",
      "6     Banker                      restock sufficient cash,   \n",
      "7     Banker                     limit the cash withdrawl,   \n",
      "8     Banker                      review all transactions,   \n",
      "9     Banker   review the credit history, apply for loans,   \n",
      "10   Manager                              operate lockers,   \n",
      "\n",
      "                                Functionality Match  \\\n",
      "0          [Verify PIN(0.88), Deposit Cheque(0.88)]   \n",
      "1           [Verify PIN(0.76), Withdraw cash(0.76)]   \n",
      "2          [Transfer Money(0.77), Verify PIN(0.63)]   \n",
      "3                  [Verify PIN(0.5), SendMail(0.5)]   \n",
      "4            [Apply loan(0.82), Credit_check(0.82)]   \n",
      "5   [Verify PIN(0.53), Check Account Balance(0.53)]   \n",
      "6         [Restock cash(0.67), Withdraw cash(0.58)]   \n",
      "7         [Withdraw cash(0.67), Restock cash(0.67)]   \n",
      "8                               [Restock cash(0.5)]   \n",
      "9            [Credit_check(0.68), Apply loan(0.55)]   \n",
      "10               [Verify PIN(0.53), SendMail(0.53)]   \n",
      "\n",
      "                                     Attributes Match  \n",
      "0   [Acc_num(0.69), Debit_pin(0.71), To_AcctNum(0....  \n",
      "1   [Max_limit(0.87), Amt_wdrl(0.77), From_AcctNum...  \n",
      "2   [From_AcctNum(0.87), Amt_wdrl(0.71), Amt_depos...  \n",
      "3   [Acc_num(0.72), From_AcctNum(0.75), Debit_pin(...  \n",
      "4   [Cust_Addr(0.6), Loan_Amt(0.68), From_AcctNum(...  \n",
      "5   [Acc_num(0.59), From_AcctNum(0.71), Debit_pin(...  \n",
      "6   [Max_limit(0.87), Amt_wdrl(0.77), From_AcctNum...  \n",
      "7   [Amt_deposit(0.75), To_AcctNum(0.66), Amt_trns...  \n",
      "8   [Amt_deposit(0.75), To_AcctNum(0.66), Amt_trns...  \n",
      "9   [Loan_purp(0.73), Loan_Amt(0.77), Amt_deposit(...  \n",
      "10  [Acc_num(0.72), From_AcctNum(0.75), Debit_pin(...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:63: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:22: FutureWarning: get_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:23: FutureWarning: get_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:79: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:85: FutureWarning: get_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:86: FutureWarning: get_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:96: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:15: FutureWarning: get_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "no_of_rows_brd = len(requirements_df.index)\n",
    "\n",
    "index = range(0,no_of_rows_brd)\n",
    "columns = ['User','Action', 'Functionality Match', 'Attributes Match']\n",
    "\n",
    "\n",
    "SimMean = pd.DataFrame(index=index, columns=columns)\n",
    "#print(requirements_df)\n",
    "SimMean.loc[0:no_of_rows_brd,'User'] = requirements_df.loc[0:no_of_rows_brd,'As a <type of user>'].values\n",
    "SimMean = extract_action_requirements_df(SimMean,requirements_df)\n",
    "SimMean = extract_bestmatch(SimMean,requirements_df,domain_df,dataelements_df)\n",
    "#SimMean = extract_bestmatch_domaintodataelem(SimMean,domain_df)\n",
    "SimMean.get_value(1,\"Attributes Match\")\n",
    "print(SimMean)\n",
    "\n",
    "writer = pd.ExcelWriter('final_output.xlsx', engine='xlsxwriter')\n",
    "SimMean.to_excel(writer, sheet_name='Sheet1')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \"*\"**********************************************************\"\n",
    "# Next steps :\n",
    "\n",
    "* Populate the correct wording in 3 sheets to provide more accurate and insightful data\n",
    "* Use OrientdB to graph the result of cosine\n",
    "* Use Node Red to start a UI dashboard\n",
    "* Optmize code to reduce memory usage\n",
    "* Move the components like Notebook,OrientDB etc to EC2 AWS ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Utility functions to store entities and relations in Orient DB **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_requirements(requirements_df):\n",
    "    \"\"\" Store requirements into the database\n",
    "    \"\"\"\n",
    "    for index, row in requirements_df.iterrows():\n",
    "        attrs = {}\n",
    "        reqid = row[\"ID\"]\n",
    "        attrs[\"Description\"] = row[\"I want to <perform some task>\"].replace('\\n', ' ').replace('\\r', '')\n",
    "        attrs[\"ID\"] = reqid\n",
    "        attrs[\"User\"]= str(row[\"As a <type of user>\"])\n",
    "        create_record(requirement_classname, reqid, attrs)    \n",
    "        \n",
    "def store_domain(domain_df):  \n",
    "    \"\"\" Store domain which has user functions into the database\n",
    "    \"\"\"\n",
    "    for index, row in domain_df.iterrows():\n",
    "        attrs = {}\n",
    "        tcaseid = row[\"ID\"]\n",
    "        attrs[\"Description\"] = row[\"Description\"].replace('\\n', ' ').replace('\\r', '')\n",
    "        attrs[\"ID\"] = tcaseid\n",
    "        attrs[\"Action\"] = str(row[\"User Function\"])\n",
    "        create_record(domain_classname, tcaseid, attrs)\n",
    "        \n",
    "def store_dataelements(dataelements_df):\n",
    "    \"\"\" Store data elements or attributes into the database\n",
    "    \"\"\"\n",
    "    for index, row in dataelements_df.iterrows():\n",
    "        attrs = {}\n",
    "        defid = row[\"ID\"]\n",
    "        attrs[\"Description\"] = row[\"Description\"].replace('\\n', ' ').replace('\\r', '')\n",
    "        attrs[\"ID\"] = defid\n",
    "        attrs[\"Short\"] = str(row[\"Short\"])\n",
    "        create_record(dataelement_classname, defid, attrs)\n",
    "        \n",
    "def store_dataelements_requirement_mapping(dataelements_df):\n",
    "    \"\"\" Store the related requirements for testcases into the database\n",
    "    \"\"\"\n",
    "    for index, row in dataelements_df.iterrows():\n",
    "        tcaseid = row[\"ID\"]\n",
    "        requirements = row[\"RequirementsMatchScore\"]\n",
    "        for requirement in requirements:\n",
    "            reqid = requirement[\"ID\"]\n",
    "            attributes = {}\n",
    "            attributes['score'] = requirement['cosine_score']\n",
    "            create_dataelements_requirement_edge(tcaseid,reqid, attributes)\n",
    "            \n",
    "def store_domain_dataelement_mapping(domain_df):\n",
    "    \"\"\" Store the related dataelement for the domain into the database\n",
    "    \"\"\"\n",
    "    for index, row in domain_df.iterrows():\n",
    "        domainid = row[\"ID\"]\n",
    "        dataelements = row[\"DataElementsMatchScore\"]\n",
    "        count = 0\n",
    "        print(\"---------\")\n",
    "        for dataelement in dataelements:\n",
    "            \n",
    "            if count < 3:\n",
    "                dataelementid = dataelement[\"ID\"]\n",
    "                attributes = {}\n",
    "                attributes['score'] = dataelement[\"cosine_score\"]\n",
    "                create_domain_dataelements_edge(domainid,dataelementid, attributes)\n",
    "                count = count + 1\n",
    "            \n",
    "def store_requirement_domain_mapping(requirements_df):\n",
    "    \"\"\" Store the related domains for the requirements in the database\n",
    "    \"\"\"\n",
    "    for index, row in requirements_df.iterrows():\n",
    "        count = 0\n",
    "        reqid = row[\"ID\"]\n",
    "        functionalities = row[\"DomainMatchScore\"]\n",
    "        print(\"----------\") \n",
    "        for functionality in functionalities:\n",
    "            \n",
    "            if count < 2:  \n",
    "                functionalityID = functionality[\"ID\"]\n",
    "                cosine_score =  functionality[\"cosine_score\"]\n",
    "                attributes = {}\n",
    "                attributes['score'] = cosine_score\n",
    "                create_requirement_domain_edge(reqid, functionalityID, attributes)\n",
    "                count = count + 1\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Store artifacts data and relations into OrientDB **\n",
    "* Drop and create a database\n",
    "* Create classes for each category of artifact\n",
    "* Store artifact data\n",
    "* Store artifact relations data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_database(\"SoftwareDesignAI\")\n",
    "create_database(\"SoftwareDesignAI\", \"admin\", \"admin\")\n",
    "\n",
    "requirement_classname = \"Requirements\"\n",
    "domain_classname = \"Domain\"\n",
    "dataelement_classname = \"DataElements\"\n",
    "\n",
    "create_class(requirement_classname)\n",
    "create_class(domain_classname)\n",
    "create_class(dataelement_classname)\n",
    "\n",
    "store_requirements(requirements_df)\n",
    "store_dataelements(dataelements_df)\n",
    "store_domain(domain_df)\n",
    "\n",
    "\n",
    "store_requirement_domain_mapping(requirements_df)\n",
    "store_domain_dataelement_mapping(domain_df)\n",
    "store_dataelements_requirement_mapping(dataelements_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Transform results for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_artifacts_mapping_d3_tree(defectId):\n",
    "    \"\"\" Create an artifacts mapping json for display by d3js tree widget\n",
    "    \"\"\"\n",
    "    depTree = {}\n",
    "    depTree['ID'] = defectId\n",
    "    testcases = get_related_testcases(defectId)\n",
    "    \n",
    "    depTree['children'] = []\n",
    "    i=1\n",
    "    for key in testcases:\n",
    "        print key,testcases[key]\n",
    "        testcaseChildren = {}\n",
    "        testcaseChildren['ID'] = key\n",
    "        testcaseChildren['Score'] = testcases[key]\n",
    "        testcaseChildren['children'] = []\n",
    "        depTree['children'].append(testcaseChildren)\n",
    "        requirements = get_related_requirements(key)\n",
    "        \n",
    "        for key in requirements:\n",
    "            requirementChildren = {}\n",
    "            requirementChildren['ID']=key\n",
    "            requirementChildren['Score']=requirements[key]\n",
    "            testcaseChildren['children'].append(requirementChildren)\n",
    "    return depTree \n",
    "\n",
    "def get_artifacts_mapping_d3_network(defectid):\n",
    "    \"\"\" Create an artifacts mapping json for display by d3js network widget\n",
    "    \"\"\"\n",
    "    nodes =[]\n",
    "    links =[] \n",
    "    defect = {}\n",
    "    defect['id'] = defectid\n",
    "    defect['group'] = 1\n",
    "    nodes.append(defect)\n",
    "    \n",
    "    testcases = get_related_testcases(defectid)\n",
    "    \n",
    "    for key in testcases:\n",
    "        testcase ={}\n",
    "        testcaseid = key\n",
    "        testcase['id'] = testcaseid\n",
    "        testcase['group'] = 2\n",
    "        if testcase not in nodes:\n",
    "            nodes.append(testcase)\n",
    "        \n",
    "        link = {}\n",
    "        link['source'] = defectid\n",
    "        link['target']=testcaseid\n",
    "        link['value']=testcases[testcaseid]\n",
    "        links.append(link)\n",
    "        \n",
    "        requirements = get_related_requirements(key)\n",
    "        for key in requirements:\n",
    "            requirement ={}\n",
    "            requirement['id'] = key\n",
    "            requirement['group'] = 3\n",
    "            if requirement not in nodes:\n",
    "                nodes.append(requirement)\n",
    "            \n",
    "            link = {}\n",
    "            link['source'] = testcaseid\n",
    "            link['target'] = key\n",
    "            link['value'] = requirements[key]\n",
    "            links.append(link)\n",
    "    result ={}\n",
    "    result[\"nodes\"] = nodes\n",
    "    result[\"links\"] = links\n",
    "    return result\n",
    "\n",
    "def get_tc_req_mapping_d3_network(testcaseid):\n",
    "    \"\"\" Create a testcases to requirement mapping json for display by d3js network widget\n",
    "    \"\"\"\n",
    "    nodes =[]\n",
    "    links =[] \n",
    "    testcase = {}\n",
    "    testcase['id'] = testcaseid\n",
    "    testcase['group'] = 2\n",
    "    nodes.append(testcase)\n",
    "    requirements = get_related_requirements(testcaseid)\n",
    "    for key in requirements:            \n",
    "        requirement ={}\n",
    "        requirement['id'] = key\n",
    "        requirement['group'] = 3\n",
    "        nodes.append(requirement)\n",
    "            \n",
    "        link = {}\n",
    "        link['source'] = testcaseid\n",
    "        link['target'] = key\n",
    "        link['value'] = requirements[key]\n",
    "        links.append(link)\n",
    "    result ={}\n",
    "    result[\"nodes\"] = nodes\n",
    "    result[\"links\"] = links\n",
    "    return result\n",
    "\n",
    "def transform_defects_d3_bubble(defects):\n",
    "    \"\"\" Transform the defects list output to a json for display by d3js bubble chart\"\"\"\n",
    "    defectsList = {}\n",
    "    defectsList['name'] = \"defect\"\n",
    "    children = []\n",
    "    for defect in defects:\n",
    "        detail = {}\n",
    "        sizeList = [400,230,130]\n",
    "        detail[\"ID\"] = defect['ID']\n",
    "        severity = int(defect['Severity'])\n",
    "        detail[\"group\"] = str(severity)\n",
    "        detail[\"size\"] = sizeList[severity-1]\n",
    "        children.append(detail)\n",
    "    defectsList['children'] = children \n",
    "    return defectsList\n",
    "\n",
    "def transform_testcases_d3_bubble(testcases):\n",
    "    \"\"\" Transform the testcases list output to a json for display by d3js bubble chart\"\"\"\n",
    "    testcasesList = {}\n",
    "    testcasesList['name'] = \"test\"\n",
    "    sizeList = {}\n",
    "    sizeList[\"FVT\"]=200\n",
    "    sizeList[\"TVT\"]=110\n",
    "    sizeList[\"SVT\"]=400\n",
    "    children = []\n",
    "    for testcase in testcases:\n",
    "        detail = {}\n",
    "        detail[\"ID\"] = testcase['ID']\n",
    "        detail[\"group\"] = testcase['Category']\n",
    "        detail[\"size\"]= sizeList[testcase['Category']]\n",
    "        children.append(detail)\n",
    "    testcasesList['children'] = children \n",
    "    return testcasesList\n",
    "\n",
    "def transform_requirements_d3_bubble(requirements):\n",
    "    \"\"\" Transform the requirements list output to a json for display by d3js bubble chart\"\"\"\n",
    "    requirementsList = {}\n",
    "    requirementsList['name'] = \"requirement\"\n",
    "    sizeList = {}\n",
    "    sizeList[1]=400\n",
    "    sizeList[2]=200\n",
    "    sizeList[3]=110\n",
    "    children = []\n",
    "    for requirement in requirements:\n",
    "        detail = {}\n",
    "        detail[\"ID\"] = requirement['ID']\n",
    "        detail[\"group\"] = requirement['Priority']\n",
    "        detail[\"size\"]= sizeList[int(requirement['Priority'])]\n",
    "        if 'defectcount' in requirement:\n",
    "            detail['defectcount'] = requirement['defectcount']\n",
    "        children.append(detail)\n",
    "    requirementsList['children'] = children \n",
    "    return requirementsList\n",
    "\n",
    "def merge_apply_filters_d3_bubble(mainList, filterList):\n",
    "    \"\"\" Add a filter attribute to the list elements for processing on UI\n",
    "    \"\"\"\n",
    "    mainListChildren = mainList['children']\n",
    "    filterListChildren = filterList['children']\n",
    "    for child in mainListChildren:\n",
    "        child['filter'] = 0\n",
    "        for child1 in filterListChildren:\n",
    "            if ( child['ID'] == child1['ID']):\n",
    "                child['filter'] = 1\n",
    "    return mainList  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Expose integration point with a websocket client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_message(ws, message):\n",
    "    print(message)\n",
    "    msg = json.loads(message)\n",
    "    print(\"message\",msg)\n",
    "    cmd = msg['cmd']\n",
    "    \n",
    "    print(\"Command :\", cmd)\n",
    "\n",
    "    if cmd == 'DefectList':\n",
    "        wsresponse = {}\n",
    "        wsresponse[\"forCmd\"] = \"DefectList\" \n",
    "        defects = get_defects()\n",
    "        wsresponse[\"response\"] = transform_defects_d3_bubble(defects)\n",
    "        ws.send(json.dumps(wsresponse))\n",
    "\n",
    "    if cmd == 'TestcaseList':\n",
    "        wsresponse = {}\n",
    "        wsresponse[\"forCmd\"] = \"TestcaseList\"\n",
    "        testcases = get_testcases()\n",
    "        wsresponse[\"response\"] = transform_testcases_d3_bubble(testcases)\n",
    "        ws.send(json.dumps(wsresponse))\n",
    "\n",
    "    if cmd == 'ReqsList':\n",
    "        wsresponse = {}\n",
    "        wsresponse[\"forCmd\"] = \"ReqsList\"\n",
    "        requirements = get_requirements()\n",
    "        wsresponse[\"response\"] = transform_requirements_d3_bubble(requirements)\n",
    "        ws.send(json.dumps(wsresponse))\n",
    "\n",
    "    if cmd == 'DefectRelation':\n",
    "        defect_id = msg['ID']\n",
    "        wsresponse = {}\n",
    "        wsresponse[\"forCmd\"] = \"DefectRelation\" \n",
    "        wsresponse[\"response\"] = get_artifacts_mapping_d3_network(defect_id)\n",
    "        ws.send(json.dumps(wsresponse))\n",
    "\n",
    "    if cmd == 'TestcaseRelation':\n",
    "        testcase_id = msg['ID']\n",
    "        wsresponse = {}\n",
    "        wsresponse[\"forCmd\"] = \"TestcaseRelation\" \n",
    "        wsresponse[\"response\"] = get_tc_req_mapping_d3_network(testcase_id)\n",
    "        ws.send(json.dumps(wsresponse))\n",
    "\n",
    "    if cmd == 'DefectInsight':\n",
    "        insight_id = msg['ID']\n",
    "        defects = get_defects()\n",
    "        defects = transform_defects_d3_bubble(defects)\n",
    "        if (insight_id.find('Insight1') != -1):\n",
    "            defectsSev1 = get_defects_severity(1)\n",
    "            defectsSev1 = transform_defects_d3_bubble(defectsSev1)\n",
    "            response = merge_apply_filters_d3_bubble(defects, defectsSev1)\n",
    "        if (insight_id.find('Insight2') != -1):\n",
    "            defectsSev2 = get_defects_severity(2)\n",
    "            defectsSev2 = transform_defects_d3_bubble(defectsSev2)\n",
    "            response = merge_apply_filters_d3_bubble(defects, defectsSev2)\n",
    "        if (insight_id.find('Insight3') != -1):\n",
    "            defectsSev3 = get_defects_severity(3)\n",
    "            defectsSev3 = transform_defects_d3_bubble(defectsSev3)\n",
    "            response = merge_apply_filters_d3_bubble(defects, defectsSev3)\n",
    "        if (insight_id.find('Insight4') != -1):\n",
    "            defects_zero_tc = get_defects_zero_testcases()\n",
    "            defects_zero_tc = transform_defects_d3_bubble(defects_zero_tc)\n",
    "            response = merge_apply_filters_d3_bubble(defects, defects_zero_tc)\n",
    "        wsresponse = {}\n",
    "        wsresponse[\"forCmd\"] = \"Insight\" \n",
    "        wsresponse[\"response\"] = response\n",
    "        ws.send(json.dumps(wsresponse))\n",
    "\n",
    "    if cmd == 'TestInsight':\n",
    "        insight_id = msg['ID']\n",
    "        testcases = get_testcases()\n",
    "        testcases = transform_testcases_d3_bubble(testcases)\n",
    "        if (insight_id.find('Insight1') != -1):\n",
    "            fvtTests = get_testcases_category('FVT')\n",
    "            fvtTests = transform_testcases_d3_bubble(fvtTests)\n",
    "            response = merge_apply_filters_d3_bubble(testcases, fvtTests)\n",
    "        if (insight_id.find('Insight2') != -1):\n",
    "            svtTests = get_testcases_category('SVT')\n",
    "            svtTests = transform_testcases_d3_bubble(svtTests)\n",
    "            response = merge_apply_filters_d3_bubble(testcases, svtTests)\n",
    "        if (insight_id.find('Insight3') != -1):\n",
    "            tvtTests = get_testcases_category('TVT')\n",
    "            tvtTests = transform_testcases_d3_bubble(tvtTests)\n",
    "            response = merge_apply_filters_d3_bubble(testcases, tvtTests)\n",
    "        if (insight_id.find('Insight4') != -1):\n",
    "            testcase_zero_defect = get_testcases_zero_defects()\n",
    "            testcase_zero_defect = transform_testcases_d3_bubble(testcase_zero_defect)\n",
    "            response = merge_apply_filters_d3_bubble(testcases, testcase_zero_defect)\n",
    "        wsresponse = {}\n",
    "        wsresponse[\"forCmd\"] = \"Insight\" \n",
    "        wsresponse[\"response\"] = response\n",
    "        ws.send(json.dumps(wsresponse))\n",
    "\n",
    "    if cmd == 'ReqInsight':\n",
    "        insight_id = msg['ID']\n",
    "        requirements = get_requirements()\n",
    "        requirements = transform_requirements_d3_bubble(requirements)\n",
    "        if (insight_id.find('Insight1') != -1):\n",
    "            req = get_requirements_zero_defect()\n",
    "            req = transform_requirements_d3_bubble(req)\n",
    "            response = merge_apply_filters_d3_bubble(requirements, req)\n",
    "        if (insight_id.find('Insight2') != -1):\n",
    "            req = get_requirements_zero_testcases()\n",
    "            req = transform_requirements_d3_bubble(req)\n",
    "            response = merge_apply_filters_d3_bubble(requirements, req)\n",
    "        if (insight_id.find('Insight3') != -1):\n",
    "            req = get_requirement_defects(5)\n",
    "            req = transform_requirements_d3_bubble(req)\n",
    "            response = merge_apply_filters_d3_bubble(requirements, req)\n",
    "        wsresponse = {}\n",
    "        wsresponse[\"forCmd\"] = \"Insight\" \n",
    "        wsresponse[\"response\"] = response\n",
    "        ws.send(json.dumps(wsresponse)) \n",
    "\n",
    "def on_error(ws, error):\n",
    "    print(error)\n",
    "\n",
    "def on_close(ws):\n",
    "    print (\"DSX Listen End\")\n",
    "    ws.send(\"DSX Listen End\")\n",
    "\n",
    "def on_open(ws):\n",
    "    def run(*args):\n",
    "        for i in range(10000):\n",
    "            hbeat = '{\"cmd\":\"EI DSX HeartBeat\"}'\n",
    "            ws.send(hbeat)\n",
    "            time.sleep(100)\n",
    "            \n",
    "    thread.start_new_thread(run, ())\n",
    "\n",
    "\n",
    "def start_websocket_listener():\n",
    "    websocket.enableTrace(True)\n",
    "    ws = websocket.WebSocketApp(\"ws://localhost:1880/ws/orchestrate\",\n",
    "                              on_message = on_message,\n",
    "                              on_error = on_error,\n",
    "                              on_close = on_close)\n",
    "    ws.on_open = on_open\n",
    "    ws.run_forever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Start websocket client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- request header ---\n",
      "GET /ws/orchestrate HTTP/1.1\n",
      "Upgrade: websocket\n",
      "Connection: Upgrade\n",
      "Host: localhost:1880\n",
      "Origin: http://localhost:1880\n",
      "Sec-WebSocket-Key: p4hJ2fd0gOaVk0OavJj1hw==\n",
      "Sec-WebSocket-Version: 13\n",
      "\n",
      "\n",
      "-----------------------\n",
      "--- response header ---\n",
      "HTTP/1.1 101 Switching Protocols\n",
      "Upgrade: websocket\n",
      "Connection: Upgrade\n",
      "Sec-WebSocket-Accept: 1pGulC36CYaOpo+5rJOZAj0Lt+E=\n",
      "-----------------------\n",
      "error from callback <function on_open at 0x1a16d86378>: name 'thread' is not defined\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/websocket/_app.py\", line 315, in _callback\n",
      "    callback(self, *args)\n",
      "  File \"<ipython-input-111-ed69d2ad5e8e>\", line 129, in on_open\n",
      "    thread.start_new_thread(run, ())\n"
     ]
    }
   ],
   "source": [
    "start_websocket_listener()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This will be used for line by line checking of code for correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "index1 = 0\n",
    "index2 = 1\n",
    "matches.append({'ID': \"U2\", \n",
    "                'cosine_score': 0, \n",
    "                'SubjectID':\"R01\"})\n",
    "cosine_score = 0.7\n",
    "matches[index2][\"cosine_score\"] = cosine_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'ID': 'U1', 'cosine_score': 0.4, 'SubjectID': 'R01'}, {'ID': 'U2', 'cosine_score': 0.7, 'SubjectID': 'R01'}]\n"
     ]
    }
   ],
   "source": [
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'ID': 'U3', 'cosine_score': 0.9, 'SubjectID': 'R03'}, {'ID': 'U4', 'cosine_score': 0.9, 'SubjectID': 'R04'}, {'ID': 'U1', 'cosine_score': 0.8, 'SubjectID': 'R01'}, {'ID': 'U2', 'cosine_score': 0.7, 'SubjectID': 'R02'}, {'ID': 'U5', 'cosine_score': 0.4, 'SubjectID': 'R05'}]\n"
     ]
    }
   ],
   "source": [
    "sorted_obj = sorted(matches, key=lambda x : x['cosine_score'], reverse=True)\n",
    "print(sorted_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "for obj in sorted_obj:\n",
    "            if obj['cosine_score'] > 0.4:\n",
    "                top_matches.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-259-382d6133efcf>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-259-382d6133efcf>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    stopWords.extend([\"The\",\"This\",\"That\",\".\",\"!\",\"?\"])print(top_matches)\u001b[0m\n\u001b[0m                                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "stopWords = get_stop_words('english')\n",
    "# List of words to be ignored for text similarity\n",
    "stopWords.extend([\"The\",\"This\",\"That\",\".\",\"!\",\"?\"])print(top_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "cosine similarity :\n"
     ]
    }
   ],
   "source": [
    "stopWords = get_stop_words('english')\n",
    "# List of words to be ignored for text similarity\n",
    "stopWords.extend([\"The\",\"This\",\"That\",\".\",\"!\",\"?\"])\n",
    "\n",
    "# this depicts cosine similarity\n",
    "\n",
    "text1 = \"A customer would like to deposit cheque at the ATM so that he can increase the balance.\"\n",
    "text1tags = ['', ' A customer', 'NP', ' cheque', ' the balance', ' like', 'VERB', ' deposit', ' increase', ' ATM']\n",
    "\n",
    "#text2 = \"This Use case deals with Verification of PIN entered by Customer at ATM. One has to validate the pin using the customer number and the debit card number\"\n",
    "#text2tags = ['', ' case', 'NP', ' the pin', ' the customer', ' number', ' the debit', ' card', ' Use', 'NAME', ' Verification', ' PIN', ' Customer', ' ATM', ' validate', 'VERB']\n",
    "\n",
    "#text1 = \"I am a Customer and I want to withdraw cash from an ATM so that I donât have to wait in line\"\n",
    "#text1tags = ['', ' cash', 'NP', ' line', ' Customer', 'NAME', ' ATM', ' withdraw', 'VERB', ' wait']\n",
    "\n",
    "#text2 = \"I am a Customer and I want to withdraw cash from an ATM so that I donât have to wait in line\"\n",
    "#text2tags = ['', ' cash', 'NP', ' line', ' Customer', 'NAME', ' ATM', ' withdraw', 'VERB', ' wait']\n",
    "\n",
    "text2 = \"Customer Name will be used as part of authenting user and to greet customer\"\n",
    "text2tags = ['', ' part', 'NP', ' user', ' customer', ' Customer Name', 'NAME', ' be', 'VERB', ' greet']\n",
    "\n",
    "\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "sentences_text1 = split_sentences(text1)\n",
    "#print(sentences_text1)\n",
    "sentences_text2 = split_sentences(text2)\n",
    "tokens_text1 = []\n",
    "tokens_text2 = []\n",
    "\n",
    "for sentence in sentences_text1:\n",
    "        tokenstemp = split_into_tokens(sentence.lower())\n",
    "        #print(tokenstemp)\n",
    "        tokens_text1.extend(tokenstemp)\n",
    "#print(tokens_text1)\n",
    "\n",
    "for sentence in sentences_text2:\n",
    "        tokenstemp = split_into_tokens(sentence.lower())\n",
    "        tokens_text2.extend(tokenstemp)\n",
    "\n",
    "if (len(text1tags) > 0):  \n",
    "        tokens_text1.extend(text1tags)\n",
    "if (len(text2tags) > 0):    \n",
    "        tokens_text2.extend(text2tags)\n",
    "#print(tokens_text1)\n",
    "        \n",
    "tokens1Filtered = [stemmer.stem(x) for x in tokens_text1 if x not in stopWords]\n",
    "#print(tokens1Filtered)    \n",
    "tokens2Filtered = [stemmer.stem(x) for x in tokens_text2 if x not in stopWords]\n",
    "    \n",
    "    #  remove duplicate tokens\n",
    "tokens1Filtered = set(tokens1Filtered)\n",
    "tokens2Filtered = set(tokens2Filtered)\n",
    "#print(tokens1Filtered)\n",
    "\n",
    "tokensList=[]\n",
    "\n",
    "text1vector = []\n",
    "text2vector = []\n",
    "    \n",
    "if len(tokens1Filtered) < len(tokens2Filtered):\n",
    "    tokensList = tokens1Filtered\n",
    "else:\n",
    "    tokensList = tokens2Filtered\n",
    "\n",
    "#print(tokensList)\n",
    "for token in tokensList:\n",
    "    if token in tokens1Filtered:\n",
    "        text1vector.append(1)\n",
    "    else:\n",
    "        text1vector.append(0)\n",
    "    if token in tokens2Filtered:\n",
    "        text2vector.append(1)\n",
    "    else:\n",
    "        text2vector.append(0)         \n",
    "#print(text1vector)  \n",
    "print(text2vector)\n",
    "cosine_similarity = 1-cosine_distance(text1vector,text2vector) \n",
    "\n",
    "print(\"cosine similarity :\")\n",
    "#print(cosine_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>As a &lt;type of user&gt;</th>\n",
       "      <th>I want to &lt;perform some task&gt;</th>\n",
       "      <th>so that I can &lt;achieve some goal&gt;</th>\n",
       "      <th>ClassifiedText</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>DomainMatchScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R01</td>\n",
       "      <td>Customer</td>\n",
       "      <td>deposit check</td>\n",
       "      <td>I want to increase my bank balance</td>\n",
       "      <td>{'Keywords': [{'User': ''}, {'User': 'Customer...</td>\n",
       "      <td>[, Customer,  deposit check, Action]</td>\n",
       "      <td>[{'ID': 'U1', 'cosine_score': 0.81649658092772...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R02</td>\n",
       "      <td>Customer</td>\n",
       "      <td>withdraw cash from an ATM</td>\n",
       "      <td>I don't have to wait in line at the Bank</td>\n",
       "      <td>{'Keywords': [{'User': ''}, {'User': 'Customer...</td>\n",
       "      <td>[, Customer,  withdraw cash, Action]</td>\n",
       "      <td>[{'ID': 'U1', 'cosine_score': 0.75592894601845...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R03</td>\n",
       "      <td>Customer</td>\n",
       "      <td>want to transfer money from one account to ano...</td>\n",
       "      <td>I don't need to pay the amount in person</td>\n",
       "      <td>{'Keywords': [{'User': ''}, {'User': 'Customer...</td>\n",
       "      <td>[, Customer,  transfer money, Action]</td>\n",
       "      <td>[{'ID': 'U4', 'cosine_score': 0.77459666924148...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>R04</td>\n",
       "      <td>Customer</td>\n",
       "      <td>pay my utility bills online</td>\n",
       "      <td>I don't need to write checks or use postal ser...</td>\n",
       "      <td>{'Keywords': [{'User': ''}, {'User': 'Customer...</td>\n",
       "      <td>[, Customer,  pay my utility bills, Action]</td>\n",
       "      <td>[{'ID': 'U1', 'cosine_score': 0.49999999999999...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>R05</td>\n",
       "      <td>Customer</td>\n",
       "      <td>apply for a loan</td>\n",
       "      <td>purchase a car</td>\n",
       "      <td>{'Keywords': [{'User': ''}, {'User': 'Customer...</td>\n",
       "      <td>[, Customer,  apply for a loan, Action]</td>\n",
       "      <td>[{'ID': 'U7', 'cosine_score': 0.81649658092772...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID As a <type of user>                      I want to <perform some task>  \\\n",
       "0  R01            Customer                                      deposit check   \n",
       "1  R02            Customer                          withdraw cash from an ATM   \n",
       "2  R03            Customer  want to transfer money from one account to ano...   \n",
       "3  R04            Customer                        pay my utility bills online   \n",
       "4  R05            Customer                                   apply for a loan   \n",
       "\n",
       "                   so that I can <achieve some goal>  \\\n",
       "0                 I want to increase my bank balance   \n",
       "1           I don't have to wait in line at the Bank   \n",
       "2           I don't need to pay the amount in person   \n",
       "3  I don't need to write checks or use postal ser...   \n",
       "4                                     purchase a car   \n",
       "\n",
       "                                      ClassifiedText  \\\n",
       "0  {'Keywords': [{'User': ''}, {'User': 'Customer...   \n",
       "1  {'Keywords': [{'User': ''}, {'User': 'Customer...   \n",
       "2  {'Keywords': [{'User': ''}, {'User': 'Customer...   \n",
       "3  {'Keywords': [{'User': ''}, {'User': 'Customer...   \n",
       "4  {'Keywords': [{'User': ''}, {'User': 'Customer...   \n",
       "\n",
       "                                      Keywords  \\\n",
       "0         [, Customer,  deposit check, Action]   \n",
       "1         [, Customer,  withdraw cash, Action]   \n",
       "2        [, Customer,  transfer money, Action]   \n",
       "3  [, Customer,  pay my utility bills, Action]   \n",
       "4      [, Customer,  apply for a loan, Action]   \n",
       "\n",
       "                                    DomainMatchScore  \n",
       "0  [{'ID': 'U1', 'cosine_score': 0.81649658092772...  \n",
       "1  [{'ID': 'U1', 'cosine_score': 0.75592894601845...  \n",
       "2  [{'ID': 'U4', 'cosine_score': 0.77459666924148...  \n",
       "3  [{'ID': 'U1', 'cosine_score': 0.49999999999999...  \n",
       "4  [{'ID': 'U7', 'cosine_score': 0.81649658092772...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requirements_df.head()\n",
    "#domain_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'usage': {'text_characters': 502, 'features': 2, 'text_units': 1}, 'keywords': [{'text': 'A', 'relevance': 0}], 'language': 'en', 'entities': [{'type': 'Person', 'text': 'Stephen Hawking', 'relevance': 0.846941, 'count': 5}]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('sample.json') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augument_SpResponse(responsejson,updateType,text,tag):\n",
    "    \"\"\" Update the NLU response JSON with augumented classifications.\n",
    "    \"\"\"\n",
    "    if(updateType == 'keyword'):\n",
    "        if not any(d.get('text', None) == text for d in responsejson['keywords']):\n",
    "            responsejson['keywords'].append({\"text\":text,\"relevance\":0.5})\n",
    "    else:\n",
    "        if not any(d.get('text', None) == text for d in responsejson['entities']):\n",
    "            responsejson['entities'].append({\"type\":tag,\"text\":text,\"relevance\":0.5,\"count\":1})        \n",
    "    return responsejson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "responsejson = augument_SpResponse(data,'entities','Ryan','Person')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'usage': {'text_characters': 502, 'features': 2, 'text_units': 1}, 'keywords': [{'text': 'A', 'relevance': 0}], 'language': 'en', 'entities': [{'type': 'Person', 'text': 'Stephen Hawking', 'relevance': 0.846941, 'count': 5}, {'type': 'Person', 'text': 'Ryan', 'relevance': 0.5, 'count': 1}]}\n"
     ]
    }
   ],
   "source": [
    "#print(responsejson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select*\n"
     ]
    }
   ],
   "source": [
    "x =\"select*\"\n",
    "\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.loads(nlu_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *****   The Following code for uploading and downloading is working\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** this will upload the file UserStroies-V0.1.xlsx in the S3\n",
    "First delete the file if you want to see it in S3 \n",
    "While downloading the png it will be downloaded as \"MYLOCALIMAGE.PNG\" in the location where you started the jupyter\n",
    "Credentials for S3 are provided above in section 2.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done uploading\n",
      "Done downloading\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from botocore.client import Config\n",
    "\n",
    "import pandas as pd\n",
    "data = None\n",
    "\n",
    "ACCESS_KEY_ID = ''\n",
    "ACCESS_SECRET_KEY = ''\n",
    "BUCKET_NAME = 'software-testing-pyscript'\n",
    "KEY = 'Banking-BRD.xlsx' # replace with your object key\n",
    "\n",
    "\n",
    "data = open('Banking-BRD.xlsx', 'rb' )\n",
    "\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=ACCESS_KEY_ID,\n",
    "    aws_secret_access_key=ACCESS_SECRET_KEY,\n",
    "    config=Config(signature_version='s3v4')\n",
    ")\n",
    "\n",
    "#s3.put_object(Bucket=BUCKET_NAME, Key=KEY, Body=data)\n",
    "\n",
    "print(\"Done uploading\")\n",
    "\n",
    "\n",
    "\n",
    "s3.download_file(BUCKET_NAME,KEY,\".//test/MYLOCALEXCELBRD_mod.xlsx\")\n",
    "    \n",
    "print(\"Done downloading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User Story ID</th>\n",
       "      <th>User</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R01</td>\n",
       "      <td>Customer</td>\n",
       "      <td>A customer would like to deposit cheque so tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R02</td>\n",
       "      <td>Customer</td>\n",
       "      <td>I am a Customer and I want to withdraw cash fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R03</td>\n",
       "      <td>Customer</td>\n",
       "      <td>Customers would like to transfer money from on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>R04</td>\n",
       "      <td>Customer</td>\n",
       "      <td>My name is Ryan and I am a customer at the ban...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>R05</td>\n",
       "      <td>Customer</td>\n",
       "      <td>Customer will need to have a feature to apply ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  User Story ID      User                                        Description\n",
       "0           R01  Customer  A customer would like to deposit cheque so tha...\n",
       "1           R02  Customer  I am a Customer and I want to withdraw cash fr...\n",
       "2           R03  Customer  Customers would like to transfer money from on...\n",
       "3           R04  Customer  My name is Ryan and I am a customer at the ban...\n",
       "4           R05  Customer  Customer will need to have a feature to apply ..."
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(\"MYLOCALEXCELBRD_mod.xlsx\",\"Banking-Requirements\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Banking-Requirements']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xl = pd.ExcelFile(\"MYLOCALEXCELBRD_mod.xlsx\")\n",
    "xl.sheet_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object DataFrame.iterrows at 0x1a133226d0>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = xl.parse(\"Banking-Requirements\")\n",
    "df.iterrows()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"configuration\": {\\n    \"classification\": {\\n      \"stages\": [\\n        {\\n          \"name\": \"Base Tagging\",\\n          \"steps\": [\\n            {\\n              \"type\": \"keywords\",\\n              \"keywords\": [\\n                {\\n                  \"tag\": \"chart\",\\n                  \"text\": \"bar\"\\n                },\\n                { \\n                  \"tag\": \"chart\",\\n                  \"text\": \"line\"\\n                },\\n                {\\n                  \"tag\": \"chart\",\\n                  \"text\": \"pie\"\\n                },\\n                {\\n                  \"tag\": \"UI\",\\n                  \"text\": \"visualization\"\\n                },\\n                {\\n                  \"tag\": \"edition\",\\n                  \"text\": \"editions\"\\n                },\\n                {\\n                  \"tag\": \"country\",\\n                  \"text\": \"countries\"\\n                },\\n                {\\n                  \"tag\": \"medal\",\\n                  \"text\": \"medals\"\\n                },\\n                {\\n                  \"tag\": \"edition\",\\n                  \"text\": \"years\"\\n                },\\n                {\\n                  \"tag\": \"login\",\\n                  \"text\": \"authentication\"\\n                },\\n                {\\n                  \"tag\": \"login\",\\n                  \"text\": \"password\"\\n                },\\n                {\\n                  \"tag\": \"login\",\\n                  \"text\": \"username\"\\n                },\\n                {\\n                  \"tag\": \"login\",\\n                  \"text\": \"credentials\"\\n                },\\n                {\\n                  \"tag\": \"websocket\",\\n                  \"text\": \"socket\"\\n                }\\n              ]\\n            },\\n            {\\n              \"type\": \"d_regex\",\\n              \"d_regex\": [\\n                {\\n                  \"tag\": \"Number\",\\n                  \"pattern\": \"[0-9]{10}\"\\n                }\\n              ]\\n            }\\n          ]\\n        }\\n      ]\\n    }\\n  }\\n}\\n'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = open(\"sample_config.txt\")\n",
    "\n",
    "config.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from xlrd import open_workbook\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "class BRD:\n",
    "    def __init__(self, usrStryID, asa, action, goal):\n",
    "        self.usrStryID = usrStryID\n",
    "        self.asa = asa\n",
    "        self.action = action\n",
    "        self.goal = goal\n",
    "        \n",
    "        #Reads the spreadsheet from the file location\n",
    "#wb = open_workbook(\"MYLOCALEXCEL.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stop-words in /anaconda3/lib/python3.6/site-packages (2015.2.23.1)\n",
      "\u001b[31mboto3 1.7.11 has requirement botocore<1.11.0,>=1.10.11, but you'll have botocore 1.10.10 which is incompatible.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    " # Prepares the list of Stop words which can be ignored like the, can , am etc\n",
    "!pip install stop-words\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "stopWords = get_stop_words('english')\n",
    "# List of words to be ignored for text similarity\n",
    "stopWords.extend([\"The\",\"This\",\"That\",\".\",\"!\",\"?\"])\n",
    "#stop_words = set(stopwords.words('english'))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/swaroopmishra/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/swaroopmishra/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Customer']\n",
      "*********************************************\n",
      "Tagged: [('Customer', 'NN')]\n",
      "----> (S Customer/NN)\n",
      "Grouped VERBS :: [Tree('S', [('Customer', 'NN')])]\n",
      "===> (S Customer/NN)\n",
      "Chunked Verbs:: [Tree('S', [('Customer', 'NN')])]\n",
      "Chunked ProNOuns:: [Tree('S', [('Customer', 'NN')])]\n",
      "['A customer would like to deposit cheque so that he can increase the balance']\n",
      "*********************************************\n",
      "Tagged: [('A', 'DT'), ('customer', 'NN'), ('would', 'MD'), ('like', 'VB'), ('to', 'TO'), ('deposit', 'VB'), ('cheque', 'NN'), ('so', 'RB'), ('that', 'IN'), ('he', 'PRP'), ('can', 'MD'), ('increase', 'VB'), ('the', 'DT'), ('balance', 'NN')]\n",
      "----> (S\n",
      "  A/DT\n",
      "  customer/NN\n",
      "  would/MD\n",
      "  (Chunk like/VB)\n",
      "  to/TO\n",
      "  (Chunk deposit/VB)\n",
      "  cheque/NN\n",
      "  so/RB\n",
      "  that/IN\n",
      "  he/PRP\n",
      "  can/MD\n",
      "  (Chunk increase/VB)\n",
      "  the/DT\n",
      "  balance/NN)\n",
      "Grouped VERBS :: [Tree('S', [('Customer', 'NN')]), Tree('S', [('A', 'DT'), ('customer', 'NN'), ('would', 'MD'), Tree('Chunk', [('like', 'VB')]), ('to', 'TO'), Tree('Chunk', [('deposit', 'VB')]), ('cheque', 'NN'), ('so', 'RB'), ('that', 'IN'), ('he', 'PRP'), ('can', 'MD'), Tree('Chunk', [('increase', 'VB')]), ('the', 'DT'), ('balance', 'NN')])]\n",
      "===> (S\n",
      "  A/DT\n",
      "  customer/NN\n",
      "  would/MD\n",
      "  like/VB\n",
      "  to/TO\n",
      "  deposit/VB\n",
      "  cheque/NN\n",
      "  so/RB\n",
      "  that/IN\n",
      "  he/PRP\n",
      "  can/MD\n",
      "  increase/VB\n",
      "  the/DT\n",
      "  balance/NN)\n",
      "Chunked Verbs:: [Tree('S', [('Customer', 'NN')]), Tree('S', [('A', 'DT'), ('customer', 'NN'), ('would', 'MD'), Tree('Chunk', [('like', 'VB')]), ('to', 'TO'), Tree('Chunk', [('deposit', 'VB')]), ('cheque', 'NN'), ('so', 'RB'), ('that', 'IN'), ('he', 'PRP'), ('can', 'MD'), Tree('Chunk', [('increase', 'VB')]), ('the', 'DT'), ('balance', 'NN')])]\n",
      "Chunked ProNOuns:: [Tree('S', [('Customer', 'NN')]), Tree('S', [('A', 'DT'), ('customer', 'NN'), ('would', 'MD'), ('like', 'VB'), ('to', 'TO'), ('deposit', 'VB'), ('cheque', 'NN'), ('so', 'RB'), ('that', 'IN'), ('he', 'PRP'), ('can', 'MD'), ('increase', 'VB'), ('the', 'DT'), ('balance', 'NN')])]\n",
      "Filtered Sentence--> ['Customer', 'A', 'customer', 'like', 'deposit', 'cheque', 'can', 'increase', 'balance']\n",
      "['Customer']\n",
      "*********************************************\n",
      "Tagged: [('Customer', 'NN')]\n",
      "----> (S Customer/NN)\n",
      "Grouped VERBS :: [Tree('S', [('Customer', 'NN')])]\n",
      "===> (S Customer/NN)\n",
      "Chunked Verbs:: [Tree('S', [('Customer', 'NN')])]\n",
      "Chunked ProNOuns:: [Tree('S', [('Customer', 'NN')])]\n",
      "['I am a Customer and I want to withdraw cash from an ATM so that I donât have to wait in line']\n",
      "*********************************************\n",
      "Tagged: [('I', 'PRP'), ('am', 'VBP'), ('a', 'DT'), ('Customer', 'NNP'), ('and', 'CC'), ('I', 'PRP'), ('want', 'VBP'), ('to', 'TO'), ('withdraw', 'VB'), ('cash', 'NN'), ('from', 'IN'), ('an', 'DT'), ('ATM', 'NNP'), ('so', 'IN'), ('that', 'IN'), ('I', 'PRP'), ('don', 'VBP'), ('â', 'JJ'), ('t', 'NNS'), ('have', 'VBP'), ('to', 'TO'), ('wait', 'VB'), ('in', 'IN'), ('line', 'NN')]\n",
      "----> (S\n",
      "  I/PRP\n",
      "  (Chunk am/VBP)\n",
      "  a/DT\n",
      "  Customer/NNP\n",
      "  and/CC\n",
      "  I/PRP\n",
      "  (Chunk want/VBP)\n",
      "  to/TO\n",
      "  (Chunk withdraw/VB)\n",
      "  cash/NN\n",
      "  from/IN\n",
      "  an/DT\n",
      "  ATM/NNP\n",
      "  so/IN\n",
      "  that/IN\n",
      "  I/PRP\n",
      "  (Chunk don/VBP)\n",
      "  â/JJ\n",
      "  t/NNS\n",
      "  (Chunk have/VBP)\n",
      "  to/TO\n",
      "  (Chunk wait/VB)\n",
      "  in/IN\n",
      "  line/NN)\n",
      "Grouped VERBS :: [Tree('S', [('Customer', 'NN')]), Tree('S', [('I', 'PRP'), Tree('Chunk', [('am', 'VBP')]), ('a', 'DT'), ('Customer', 'NNP'), ('and', 'CC'), ('I', 'PRP'), Tree('Chunk', [('want', 'VBP')]), ('to', 'TO'), Tree('Chunk', [('withdraw', 'VB')]), ('cash', 'NN'), ('from', 'IN'), ('an', 'DT'), ('ATM', 'NNP'), ('so', 'IN'), ('that', 'IN'), ('I', 'PRP'), Tree('Chunk', [('don', 'VBP')]), ('â', 'JJ'), ('t', 'NNS'), Tree('Chunk', [('have', 'VBP')]), ('to', 'TO'), Tree('Chunk', [('wait', 'VB')]), ('in', 'IN'), ('line', 'NN')])]\n",
      "===> (S\n",
      "  I/PRP\n",
      "  am/VBP\n",
      "  a/DT\n",
      "  (Chunk Customer/NNP)\n",
      "  and/CC\n",
      "  I/PRP\n",
      "  want/VBP\n",
      "  to/TO\n",
      "  withdraw/VB\n",
      "  cash/NN\n",
      "  from/IN\n",
      "  an/DT\n",
      "  (Chunk ATM/NNP)\n",
      "  so/IN\n",
      "  that/IN\n",
      "  I/PRP\n",
      "  don/VBP\n",
      "  â/JJ\n",
      "  t/NNS\n",
      "  have/VBP\n",
      "  to/TO\n",
      "  wait/VB\n",
      "  in/IN\n",
      "  line/NN)\n",
      "Chunked Verbs:: [Tree('S', [('Customer', 'NN')]), Tree('S', [('I', 'PRP'), Tree('Chunk', [('am', 'VBP')]), ('a', 'DT'), ('Customer', 'NNP'), ('and', 'CC'), ('I', 'PRP'), Tree('Chunk', [('want', 'VBP')]), ('to', 'TO'), Tree('Chunk', [('withdraw', 'VB')]), ('cash', 'NN'), ('from', 'IN'), ('an', 'DT'), ('ATM', 'NNP'), ('so', 'IN'), ('that', 'IN'), ('I', 'PRP'), Tree('Chunk', [('don', 'VBP')]), ('â', 'JJ'), ('t', 'NNS'), Tree('Chunk', [('have', 'VBP')]), ('to', 'TO'), Tree('Chunk', [('wait', 'VB')]), ('in', 'IN'), ('line', 'NN')])]\n",
      "Chunked ProNOuns:: [Tree('S', [('Customer', 'NN')]), Tree('S', [('I', 'PRP'), ('am', 'VBP'), ('a', 'DT'), Tree('Chunk', [('Customer', 'NNP')]), ('and', 'CC'), ('I', 'PRP'), ('want', 'VBP'), ('to', 'TO'), ('withdraw', 'VB'), ('cash', 'NN'), ('from', 'IN'), ('an', 'DT'), Tree('Chunk', [('ATM', 'NNP')]), ('so', 'IN'), ('that', 'IN'), ('I', 'PRP'), ('don', 'VBP'), ('â', 'JJ'), ('t', 'NNS'), ('have', 'VBP'), ('to', 'TO'), ('wait', 'VB'), ('in', 'IN'), ('line', 'NN')])]\n",
      "Filtered Sentence--> ['Customer', 'I', 'Customer', 'I', 'want', 'withdraw', 'cash', 'ATM', 'I', 'don', 'â', 't', 'wait', 'line']\n",
      "['Customer']\n",
      "*********************************************\n",
      "Tagged: [('Customer', 'NN')]\n",
      "----> (S Customer/NN)\n",
      "Grouped VERBS :: [Tree('S', [('Customer', 'NN')])]\n",
      "===> (S Customer/NN)\n",
      "Chunked Verbs:: [Tree('S', [('Customer', 'NN')])]\n",
      "Chunked ProNOuns:: [Tree('S', [('Customer', 'NN')])]\n",
      "['Customers would like to transfer money from one account to another so that there is no physical transfer of money']\n",
      "*********************************************\n",
      "Tagged: [('Customers', 'NNS'), ('would', 'MD'), ('like', 'VB'), ('to', 'TO'), ('transfer', 'VB'), ('money', 'NN'), ('from', 'IN'), ('one', 'CD'), ('account', 'NN'), ('to', 'TO'), ('another', 'DT'), ('so', 'RB'), ('that', 'IN'), ('there', 'EX'), ('is', 'VBZ'), ('no', 'DT'), ('physical', 'JJ'), ('transfer', 'NN'), ('of', 'IN'), ('money', 'NN')]\n",
      "----> (S\n",
      "  Customers/NNS\n",
      "  would/MD\n",
      "  (Chunk like/VB)\n",
      "  to/TO\n",
      "  (Chunk transfer/VB)\n",
      "  money/NN\n",
      "  from/IN\n",
      "  one/CD\n",
      "  account/NN\n",
      "  to/TO\n",
      "  another/DT\n",
      "  so/RB\n",
      "  that/IN\n",
      "  there/EX\n",
      "  (Chunk is/VBZ)\n",
      "  no/DT\n",
      "  physical/JJ\n",
      "  transfer/NN\n",
      "  of/IN\n",
      "  money/NN)\n",
      "Grouped VERBS :: [Tree('S', [('Customer', 'NN')]), Tree('S', [('Customers', 'NNS'), ('would', 'MD'), Tree('Chunk', [('like', 'VB')]), ('to', 'TO'), Tree('Chunk', [('transfer', 'VB')]), ('money', 'NN'), ('from', 'IN'), ('one', 'CD'), ('account', 'NN'), ('to', 'TO'), ('another', 'DT'), ('so', 'RB'), ('that', 'IN'), ('there', 'EX'), Tree('Chunk', [('is', 'VBZ')]), ('no', 'DT'), ('physical', 'JJ'), ('transfer', 'NN'), ('of', 'IN'), ('money', 'NN')])]\n",
      "===> (S\n",
      "  Customers/NNS\n",
      "  would/MD\n",
      "  like/VB\n",
      "  to/TO\n",
      "  transfer/VB\n",
      "  money/NN\n",
      "  from/IN\n",
      "  one/CD\n",
      "  account/NN\n",
      "  to/TO\n",
      "  another/DT\n",
      "  so/RB\n",
      "  that/IN\n",
      "  there/EX\n",
      "  is/VBZ\n",
      "  no/DT\n",
      "  physical/JJ\n",
      "  transfer/NN\n",
      "  of/IN\n",
      "  money/NN)\n",
      "Chunked Verbs:: [Tree('S', [('Customer', 'NN')]), Tree('S', [('Customers', 'NNS'), ('would', 'MD'), Tree('Chunk', [('like', 'VB')]), ('to', 'TO'), Tree('Chunk', [('transfer', 'VB')]), ('money', 'NN'), ('from', 'IN'), ('one', 'CD'), ('account', 'NN'), ('to', 'TO'), ('another', 'DT'), ('so', 'RB'), ('that', 'IN'), ('there', 'EX'), Tree('Chunk', [('is', 'VBZ')]), ('no', 'DT'), ('physical', 'JJ'), ('transfer', 'NN'), ('of', 'IN'), ('money', 'NN')])]\n",
      "Chunked ProNOuns:: [Tree('S', [('Customer', 'NN')]), Tree('S', [('Customers', 'NNS'), ('would', 'MD'), ('like', 'VB'), ('to', 'TO'), ('transfer', 'VB'), ('money', 'NN'), ('from', 'IN'), ('one', 'CD'), ('account', 'NN'), ('to', 'TO'), ('another', 'DT'), ('so', 'RB'), ('that', 'IN'), ('there', 'EX'), ('is', 'VBZ'), ('no', 'DT'), ('physical', 'JJ'), ('transfer', 'NN'), ('of', 'IN'), ('money', 'NN')])]\n",
      "Filtered Sentence--> ['Customer', 'Customers', 'like', 'transfer', 'money', 'one', 'account', 'another', 'physical', 'transfer', 'money']\n",
      "['Customer']\n",
      "*********************************************\n",
      "Tagged: [('Customer', 'NN')]\n",
      "----> (S Customer/NN)\n",
      "Grouped VERBS :: [Tree('S', [('Customer', 'NN')])]\n",
      "===> (S Customer/NN)\n",
      "Chunked Verbs:: [Tree('S', [('Customer', 'NN')])]\n",
      "Chunked ProNOuns:: [Tree('S', [('Customer', 'NN')])]\n",
      "['My name is Ryan and I am a customer at the bank, and I want to order checks so that I donât need to write checks or use postal service']\n",
      "*********************************************\n",
      "Tagged: [('My', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), ('Ryan', 'NNP'), ('and', 'CC'), ('I', 'PRP'), ('am', 'VBP'), ('a', 'DT'), ('customer', 'NN'), ('at', 'IN'), ('the', 'DT'), ('bank', 'NN'), (',', ','), ('and', 'CC'), ('I', 'PRP'), ('want', 'VBP'), ('to', 'TO'), ('order', 'NN'), ('checks', 'NNS'), ('so', 'IN'), ('that', 'IN'), ('I', 'PRP'), ('don', 'VBP'), ('â', 'JJ'), ('t', 'NNS'), ('need', 'VBP'), ('to', 'TO'), ('write', 'VB'), ('checks', 'NNS'), ('or', 'CC'), ('use', 'VB'), ('postal', 'JJ'), ('service', 'NN')]\n",
      "----> (S\n",
      "  My/PRP$\n",
      "  name/NN\n",
      "  (Chunk is/VBZ)\n",
      "  Ryan/NNP\n",
      "  and/CC\n",
      "  I/PRP\n",
      "  (Chunk am/VBP)\n",
      "  a/DT\n",
      "  customer/NN\n",
      "  at/IN\n",
      "  the/DT\n",
      "  bank/NN\n",
      "  ,/,\n",
      "  and/CC\n",
      "  I/PRP\n",
      "  (Chunk want/VBP)\n",
      "  to/TO\n",
      "  order/NN\n",
      "  checks/NNS\n",
      "  so/IN\n",
      "  that/IN\n",
      "  I/PRP\n",
      "  (Chunk don/VBP)\n",
      "  â/JJ\n",
      "  t/NNS\n",
      "  (Chunk need/VBP)\n",
      "  to/TO\n",
      "  (Chunk write/VB)\n",
      "  checks/NNS\n",
      "  or/CC\n",
      "  (Chunk use/VB)\n",
      "  postal/JJ\n",
      "  service/NN)\n",
      "Grouped VERBS :: [Tree('S', [('Customer', 'NN')]), Tree('S', [('My', 'PRP$'), ('name', 'NN'), Tree('Chunk', [('is', 'VBZ')]), ('Ryan', 'NNP'), ('and', 'CC'), ('I', 'PRP'), Tree('Chunk', [('am', 'VBP')]), ('a', 'DT'), ('customer', 'NN'), ('at', 'IN'), ('the', 'DT'), ('bank', 'NN'), (',', ','), ('and', 'CC'), ('I', 'PRP'), Tree('Chunk', [('want', 'VBP')]), ('to', 'TO'), ('order', 'NN'), ('checks', 'NNS'), ('so', 'IN'), ('that', 'IN'), ('I', 'PRP'), Tree('Chunk', [('don', 'VBP')]), ('â', 'JJ'), ('t', 'NNS'), Tree('Chunk', [('need', 'VBP')]), ('to', 'TO'), Tree('Chunk', [('write', 'VB')]), ('checks', 'NNS'), ('or', 'CC'), Tree('Chunk', [('use', 'VB')]), ('postal', 'JJ'), ('service', 'NN')])]\n",
      "===> (S\n",
      "  My/PRP$\n",
      "  name/NN\n",
      "  is/VBZ\n",
      "  (Chunk Ryan/NNP)\n",
      "  and/CC\n",
      "  I/PRP\n",
      "  am/VBP\n",
      "  a/DT\n",
      "  customer/NN\n",
      "  at/IN\n",
      "  the/DT\n",
      "  bank/NN\n",
      "  ,/,\n",
      "  and/CC\n",
      "  I/PRP\n",
      "  want/VBP\n",
      "  to/TO\n",
      "  order/NN\n",
      "  checks/NNS\n",
      "  so/IN\n",
      "  that/IN\n",
      "  I/PRP\n",
      "  don/VBP\n",
      "  â/JJ\n",
      "  t/NNS\n",
      "  need/VBP\n",
      "  to/TO\n",
      "  write/VB\n",
      "  checks/NNS\n",
      "  or/CC\n",
      "  use/VB\n",
      "  postal/JJ\n",
      "  service/NN)\n",
      "Chunked Verbs:: [Tree('S', [('Customer', 'NN')]), Tree('S', [('My', 'PRP$'), ('name', 'NN'), Tree('Chunk', [('is', 'VBZ')]), ('Ryan', 'NNP'), ('and', 'CC'), ('I', 'PRP'), Tree('Chunk', [('am', 'VBP')]), ('a', 'DT'), ('customer', 'NN'), ('at', 'IN'), ('the', 'DT'), ('bank', 'NN'), (',', ','), ('and', 'CC'), ('I', 'PRP'), Tree('Chunk', [('want', 'VBP')]), ('to', 'TO'), ('order', 'NN'), ('checks', 'NNS'), ('so', 'IN'), ('that', 'IN'), ('I', 'PRP'), Tree('Chunk', [('don', 'VBP')]), ('â', 'JJ'), ('t', 'NNS'), Tree('Chunk', [('need', 'VBP')]), ('to', 'TO'), Tree('Chunk', [('write', 'VB')]), ('checks', 'NNS'), ('or', 'CC'), Tree('Chunk', [('use', 'VB')]), ('postal', 'JJ'), ('service', 'NN')])]\n",
      "Chunked ProNOuns:: [Tree('S', [('Customer', 'NN')]), Tree('S', [('My', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), Tree('Chunk', [('Ryan', 'NNP')]), ('and', 'CC'), ('I', 'PRP'), ('am', 'VBP'), ('a', 'DT'), ('customer', 'NN'), ('at', 'IN'), ('the', 'DT'), ('bank', 'NN'), (',', ','), ('and', 'CC'), ('I', 'PRP'), ('want', 'VBP'), ('to', 'TO'), ('order', 'NN'), ('checks', 'NNS'), ('so', 'IN'), ('that', 'IN'), ('I', 'PRP'), ('don', 'VBP'), ('â', 'JJ'), ('t', 'NNS'), ('need', 'VBP'), ('to', 'TO'), ('write', 'VB'), ('checks', 'NNS'), ('or', 'CC'), ('use', 'VB'), ('postal', 'JJ'), ('service', 'NN')])]\n",
      "Filtered Sentence--> ['Customer', 'My', 'name', 'Ryan', 'I', 'customer', 'bank', ',', 'I', 'want', 'order', 'checks', 'I', 'don', 'â', 't', 'need', 'write', 'checks', 'use', 'postal', 'service']\n",
      "['Customer']\n",
      "*********************************************\n",
      "Tagged: [('Customer', 'NN')]\n",
      "----> (S Customer/NN)\n",
      "Grouped VERBS :: [Tree('S', [('Customer', 'NN')])]\n",
      "===> (S Customer/NN)\n",
      "Chunked Verbs:: [Tree('S', [('Customer', 'NN')])]\n",
      "Chunked ProNOuns:: [Tree('S', [('Customer', 'NN')])]\n",
      "['Customer will need to have a feature to apply for a loan from bank so that he can purchase a car']\n",
      "*********************************************\n",
      "Tagged: [('Customer', 'NNP'), ('will', 'MD'), ('need', 'VB'), ('to', 'TO'), ('have', 'VB'), ('a', 'DT'), ('feature', 'NN'), ('to', 'TO'), ('apply', 'VB'), ('for', 'IN'), ('a', 'DT'), ('loan', 'NN'), ('from', 'IN'), ('bank', 'NN'), ('so', 'IN'), ('that', 'IN'), ('he', 'PRP'), ('can', 'MD'), ('purchase', 'VB'), ('a', 'DT'), ('car', 'NN')]\n",
      "----> (S\n",
      "  Customer/NNP\n",
      "  will/MD\n",
      "  (Chunk need/VB)\n",
      "  to/TO\n",
      "  (Chunk have/VB)\n",
      "  a/DT\n",
      "  feature/NN\n",
      "  to/TO\n",
      "  (Chunk apply/VB)\n",
      "  for/IN\n",
      "  a/DT\n",
      "  loan/NN\n",
      "  from/IN\n",
      "  bank/NN\n",
      "  so/IN\n",
      "  that/IN\n",
      "  he/PRP\n",
      "  can/MD\n",
      "  (Chunk purchase/VB)\n",
      "  a/DT\n",
      "  car/NN)\n",
      "Grouped VERBS :: [Tree('S', [('Customer', 'NN')]), Tree('S', [('Customer', 'NNP'), ('will', 'MD'), Tree('Chunk', [('need', 'VB')]), ('to', 'TO'), Tree('Chunk', [('have', 'VB')]), ('a', 'DT'), ('feature', 'NN'), ('to', 'TO'), Tree('Chunk', [('apply', 'VB')]), ('for', 'IN'), ('a', 'DT'), ('loan', 'NN'), ('from', 'IN'), ('bank', 'NN'), ('so', 'IN'), ('that', 'IN'), ('he', 'PRP'), ('can', 'MD'), Tree('Chunk', [('purchase', 'VB')]), ('a', 'DT'), ('car', 'NN')])]\n",
      "===> (S\n",
      "  (Chunk Customer/NNP)\n",
      "  will/MD\n",
      "  need/VB\n",
      "  to/TO\n",
      "  have/VB\n",
      "  a/DT\n",
      "  feature/NN\n",
      "  to/TO\n",
      "  apply/VB\n",
      "  for/IN\n",
      "  a/DT\n",
      "  loan/NN\n",
      "  from/IN\n",
      "  bank/NN\n",
      "  so/IN\n",
      "  that/IN\n",
      "  he/PRP\n",
      "  can/MD\n",
      "  purchase/VB\n",
      "  a/DT\n",
      "  car/NN)\n",
      "Chunked Verbs:: [Tree('S', [('Customer', 'NN')]), Tree('S', [('Customer', 'NNP'), ('will', 'MD'), Tree('Chunk', [('need', 'VB')]), ('to', 'TO'), Tree('Chunk', [('have', 'VB')]), ('a', 'DT'), ('feature', 'NN'), ('to', 'TO'), Tree('Chunk', [('apply', 'VB')]), ('for', 'IN'), ('a', 'DT'), ('loan', 'NN'), ('from', 'IN'), ('bank', 'NN'), ('so', 'IN'), ('that', 'IN'), ('he', 'PRP'), ('can', 'MD'), Tree('Chunk', [('purchase', 'VB')]), ('a', 'DT'), ('car', 'NN')])]\n",
      "Chunked ProNOuns:: [Tree('S', [('Customer', 'NN')]), Tree('S', [Tree('Chunk', [('Customer', 'NNP')]), ('will', 'MD'), ('need', 'VB'), ('to', 'TO'), ('have', 'VB'), ('a', 'DT'), ('feature', 'NN'), ('to', 'TO'), ('apply', 'VB'), ('for', 'IN'), ('a', 'DT'), ('loan', 'NN'), ('from', 'IN'), ('bank', 'NN'), ('so', 'IN'), ('that', 'IN'), ('he', 'PRP'), ('can', 'MD'), ('purchase', 'VB'), ('a', 'DT'), ('car', 'NN')])]\n",
      "Filtered Sentence--> ['Customer', 'Customer', 'will', 'need', 'feature', 'apply', 'loan', 'bank', 'can', 'purchase', 'car']\n",
      "['Banker']\n",
      "*********************************************\n",
      "Tagged: [('Banker', 'NN')]\n",
      "----> (S Banker/NN)\n",
      "Grouped VERBS :: [Tree('S', [('Banker', 'NN')])]\n",
      "===> (S Banker/NN)\n",
      "Chunked Verbs:: [Tree('S', [('Banker', 'NN')])]\n",
      "Chunked ProNOuns:: [Tree('S', [('Banker', 'NN')])]\n",
      "['Banker should be able to restock sufficient cash in ATM.', 'This will help customer to withdraw cash.']\n",
      "*********************************************\n",
      "Tagged: [('Banker', 'NNP'), ('should', 'MD'), ('be', 'VB'), ('able', 'JJ'), ('to', 'TO'), ('restock', 'VB'), ('sufficient', 'JJ'), ('cash', 'NN'), ('in', 'IN'), ('ATM', 'NNP'), ('.', '.'), ('This', 'DT'), ('will', 'MD'), ('help', 'VB'), ('customer', 'NN'), ('to', 'TO'), ('withdraw', 'VB'), ('cash', 'NN'), ('.', '.')]\n",
      "----> (S\n",
      "  Banker/NNP\n",
      "  should/MD\n",
      "  (Chunk be/VB)\n",
      "  able/JJ\n",
      "  to/TO\n",
      "  (Chunk restock/VB)\n",
      "  sufficient/JJ\n",
      "  cash/NN\n",
      "  in/IN\n",
      "  ATM/NNP\n",
      "  ./.\n",
      "  This/DT\n",
      "  will/MD\n",
      "  (Chunk help/VB)\n",
      "  customer/NN\n",
      "  to/TO\n",
      "  (Chunk withdraw/VB)\n",
      "  cash/NN\n",
      "  ./.)\n",
      "Grouped VERBS :: [Tree('S', [('Banker', 'NN')]), Tree('S', [('Banker', 'NNP'), ('should', 'MD'), Tree('Chunk', [('be', 'VB')]), ('able', 'JJ'), ('to', 'TO'), Tree('Chunk', [('restock', 'VB')]), ('sufficient', 'JJ'), ('cash', 'NN'), ('in', 'IN'), ('ATM', 'NNP'), ('.', '.'), ('This', 'DT'), ('will', 'MD'), Tree('Chunk', [('help', 'VB')]), ('customer', 'NN'), ('to', 'TO'), Tree('Chunk', [('withdraw', 'VB')]), ('cash', 'NN'), ('.', '.')])]\n",
      "===> (S\n",
      "  (Chunk Banker/NNP)\n",
      "  should/MD\n",
      "  be/VB\n",
      "  able/JJ\n",
      "  to/TO\n",
      "  restock/VB\n",
      "  sufficient/JJ\n",
      "  cash/NN\n",
      "  in/IN\n",
      "  (Chunk ATM/NNP)\n",
      "  ./.\n",
      "  This/DT\n",
      "  will/MD\n",
      "  help/VB\n",
      "  customer/NN\n",
      "  to/TO\n",
      "  withdraw/VB\n",
      "  cash/NN\n",
      "  ./.)\n",
      "Chunked Verbs:: [Tree('S', [('Banker', 'NN')]), Tree('S', [('Banker', 'NNP'), ('should', 'MD'), Tree('Chunk', [('be', 'VB')]), ('able', 'JJ'), ('to', 'TO'), Tree('Chunk', [('restock', 'VB')]), ('sufficient', 'JJ'), ('cash', 'NN'), ('in', 'IN'), ('ATM', 'NNP'), ('.', '.'), ('This', 'DT'), ('will', 'MD'), Tree('Chunk', [('help', 'VB')]), ('customer', 'NN'), ('to', 'TO'), Tree('Chunk', [('withdraw', 'VB')]), ('cash', 'NN'), ('.', '.')])]\n",
      "Chunked ProNOuns:: [Tree('S', [('Banker', 'NN')]), Tree('S', [Tree('Chunk', [('Banker', 'NNP')]), ('should', 'MD'), ('be', 'VB'), ('able', 'JJ'), ('to', 'TO'), ('restock', 'VB'), ('sufficient', 'JJ'), ('cash', 'NN'), ('in', 'IN'), Tree('Chunk', [('ATM', 'NNP')]), ('.', '.'), ('This', 'DT'), ('will', 'MD'), ('help', 'VB'), ('customer', 'NN'), ('to', 'TO'), ('withdraw', 'VB'), ('cash', 'NN'), ('.', '.')])]\n",
      "Filtered Sentence--> ['Banker', 'Banker', 'able', 'restock', 'sufficient', 'cash', 'ATM', 'will', 'help', 'customer', 'withdraw', 'cash']\n",
      "['Banker']\n",
      "*********************************************\n",
      "Tagged: [('Banker', 'NN')]\n",
      "----> (S Banker/NN)\n",
      "Grouped VERBS :: [Tree('S', [('Banker', 'NN')])]\n",
      "===> (S Banker/NN)\n",
      "Chunked Verbs:: [Tree('S', [('Banker', 'NN')])]\n",
      "Chunked ProNOuns:: [Tree('S', [('Banker', 'NN')])]\n",
      "['Banker will need to limit the cash withdrwal from ATM.', 'This will help more customers can prevail the ATM cash and also prevent fradulent activities']\n",
      "*********************************************\n",
      "Tagged: [('Banker', 'NNP'), ('will', 'MD'), ('need', 'VB'), ('to', 'TO'), ('limit', 'VB'), ('the', 'DT'), ('cash', 'NN'), ('withdrwal', 'NN'), ('from', 'IN'), ('ATM', 'NNP'), ('.', '.'), ('This', 'DT'), ('will', 'MD'), ('help', 'VB'), ('more', 'JJR'), ('customers', 'NNS'), ('can', 'MD'), ('prevail', 'VB'), ('the', 'DT'), ('ATM', 'NNP'), ('cash', 'NN'), ('and', 'CC'), ('also', 'RB'), ('prevent', 'JJ'), ('fradulent', 'NN'), ('activities', 'NNS')]\n",
      "----> (S\n",
      "  Banker/NNP\n",
      "  will/MD\n",
      "  (Chunk need/VB)\n",
      "  to/TO\n",
      "  (Chunk limit/VB)\n",
      "  the/DT\n",
      "  cash/NN\n",
      "  withdrwal/NN\n",
      "  from/IN\n",
      "  ATM/NNP\n",
      "  ./.\n",
      "  This/DT\n",
      "  will/MD\n",
      "  (Chunk help/VB)\n",
      "  more/JJR\n",
      "  customers/NNS\n",
      "  can/MD\n",
      "  (Chunk prevail/VB)\n",
      "  the/DT\n",
      "  ATM/NNP\n",
      "  cash/NN\n",
      "  and/CC\n",
      "  also/RB\n",
      "  prevent/JJ\n",
      "  fradulent/NN\n",
      "  activities/NNS)\n",
      "Grouped VERBS :: [Tree('S', [('Banker', 'NN')]), Tree('S', [('Banker', 'NNP'), ('will', 'MD'), Tree('Chunk', [('need', 'VB')]), ('to', 'TO'), Tree('Chunk', [('limit', 'VB')]), ('the', 'DT'), ('cash', 'NN'), ('withdrwal', 'NN'), ('from', 'IN'), ('ATM', 'NNP'), ('.', '.'), ('This', 'DT'), ('will', 'MD'), Tree('Chunk', [('help', 'VB')]), ('more', 'JJR'), ('customers', 'NNS'), ('can', 'MD'), Tree('Chunk', [('prevail', 'VB')]), ('the', 'DT'), ('ATM', 'NNP'), ('cash', 'NN'), ('and', 'CC'), ('also', 'RB'), ('prevent', 'JJ'), ('fradulent', 'NN'), ('activities', 'NNS')])]\n",
      "===> (S\n",
      "  (Chunk Banker/NNP)\n",
      "  will/MD\n",
      "  need/VB\n",
      "  to/TO\n",
      "  limit/VB\n",
      "  the/DT\n",
      "  cash/NN\n",
      "  withdrwal/NN\n",
      "  from/IN\n",
      "  (Chunk ATM/NNP)\n",
      "  ./.\n",
      "  This/DT\n",
      "  will/MD\n",
      "  help/VB\n",
      "  more/JJR\n",
      "  customers/NNS\n",
      "  can/MD\n",
      "  prevail/VB\n",
      "  the/DT\n",
      "  (Chunk ATM/NNP)\n",
      "  cash/NN\n",
      "  and/CC\n",
      "  also/RB\n",
      "  prevent/JJ\n",
      "  fradulent/NN\n",
      "  activities/NNS)\n",
      "Chunked Verbs:: [Tree('S', [('Banker', 'NN')]), Tree('S', [('Banker', 'NNP'), ('will', 'MD'), Tree('Chunk', [('need', 'VB')]), ('to', 'TO'), Tree('Chunk', [('limit', 'VB')]), ('the', 'DT'), ('cash', 'NN'), ('withdrwal', 'NN'), ('from', 'IN'), ('ATM', 'NNP'), ('.', '.'), ('This', 'DT'), ('will', 'MD'), Tree('Chunk', [('help', 'VB')]), ('more', 'JJR'), ('customers', 'NNS'), ('can', 'MD'), Tree('Chunk', [('prevail', 'VB')]), ('the', 'DT'), ('ATM', 'NNP'), ('cash', 'NN'), ('and', 'CC'), ('also', 'RB'), ('prevent', 'JJ'), ('fradulent', 'NN'), ('activities', 'NNS')])]\n",
      "Chunked ProNOuns:: [Tree('S', [('Banker', 'NN')]), Tree('S', [Tree('Chunk', [('Banker', 'NNP')]), ('will', 'MD'), ('need', 'VB'), ('to', 'TO'), ('limit', 'VB'), ('the', 'DT'), ('cash', 'NN'), ('withdrwal', 'NN'), ('from', 'IN'), Tree('Chunk', [('ATM', 'NNP')]), ('.', '.'), ('This', 'DT'), ('will', 'MD'), ('help', 'VB'), ('more', 'JJR'), ('customers', 'NNS'), ('can', 'MD'), ('prevail', 'VB'), ('the', 'DT'), Tree('Chunk', [('ATM', 'NNP')]), ('cash', 'NN'), ('and', 'CC'), ('also', 'RB'), ('prevent', 'JJ'), ('fradulent', 'NN'), ('activities', 'NNS')])]\n",
      "Filtered Sentence--> ['Banker', 'Banker', 'will', 'need', 'limit', 'cash', 'withdrwal', 'ATM', 'will', 'help', 'customers', 'can', 'prevail', 'ATM', 'cash', 'also', 'prevent', 'fradulent', 'activities']\n",
      "['Banker']\n",
      "*********************************************\n",
      "Tagged: [('Banker', 'NN')]\n",
      "----> (S Banker/NN)\n",
      "Grouped VERBS :: [Tree('S', [('Banker', 'NN')])]\n",
      "===> (S Banker/NN)\n",
      "Chunked Verbs:: [Tree('S', [('Banker', 'NN')])]\n",
      "Chunked ProNOuns:: [Tree('S', [('Banker', 'NN')])]\n",
      "['Banker should be able to review the credit history and the bank balance of customers that apply for loans .', 'This will help banker to decide wether to approve or reject the loan applications']\n",
      "*********************************************\n",
      "Tagged: [('Banker', 'NNP'), ('should', 'MD'), ('be', 'VB'), ('able', 'JJ'), ('to', 'TO'), ('review', 'VB'), ('the', 'DT'), ('credit', 'NN'), ('history', 'NN'), ('and', 'CC'), ('the', 'DT'), ('bank', 'NN'), ('balance', 'NN'), ('of', 'IN'), ('customers', 'NNS'), ('that', 'WDT'), ('apply', 'VBP'), ('for', 'IN'), ('loans', 'NNS'), ('.', '.'), ('This', 'DT'), ('will', 'MD'), ('help', 'VB'), ('banker', 'NN'), ('to', 'TO'), ('decide', 'VB'), ('wether', 'JJR'), ('to', 'TO'), ('approve', 'VB'), ('or', 'CC'), ('reject', 'VB'), ('the', 'DT'), ('loan', 'NN'), ('applications', 'NNS')]\n",
      "----> (S\n",
      "  Banker/NNP\n",
      "  should/MD\n",
      "  (Chunk be/VB)\n",
      "  able/JJ\n",
      "  to/TO\n",
      "  (Chunk review/VB)\n",
      "  the/DT\n",
      "  credit/NN\n",
      "  history/NN\n",
      "  and/CC\n",
      "  the/DT\n",
      "  bank/NN\n",
      "  balance/NN\n",
      "  of/IN\n",
      "  customers/NNS\n",
      "  that/WDT\n",
      "  (Chunk apply/VBP)\n",
      "  for/IN\n",
      "  loans/NNS\n",
      "  ./.\n",
      "  This/DT\n",
      "  will/MD\n",
      "  (Chunk help/VB)\n",
      "  banker/NN\n",
      "  to/TO\n",
      "  (Chunk decide/VB)\n",
      "  wether/JJR\n",
      "  to/TO\n",
      "  (Chunk approve/VB)\n",
      "  or/CC\n",
      "  (Chunk reject/VB)\n",
      "  the/DT\n",
      "  loan/NN\n",
      "  applications/NNS)\n",
      "Grouped VERBS :: [Tree('S', [('Banker', 'NN')]), Tree('S', [('Banker', 'NNP'), ('should', 'MD'), Tree('Chunk', [('be', 'VB')]), ('able', 'JJ'), ('to', 'TO'), Tree('Chunk', [('review', 'VB')]), ('the', 'DT'), ('credit', 'NN'), ('history', 'NN'), ('and', 'CC'), ('the', 'DT'), ('bank', 'NN'), ('balance', 'NN'), ('of', 'IN'), ('customers', 'NNS'), ('that', 'WDT'), Tree('Chunk', [('apply', 'VBP')]), ('for', 'IN'), ('loans', 'NNS'), ('.', '.'), ('This', 'DT'), ('will', 'MD'), Tree('Chunk', [('help', 'VB')]), ('banker', 'NN'), ('to', 'TO'), Tree('Chunk', [('decide', 'VB')]), ('wether', 'JJR'), ('to', 'TO'), Tree('Chunk', [('approve', 'VB')]), ('or', 'CC'), Tree('Chunk', [('reject', 'VB')]), ('the', 'DT'), ('loan', 'NN'), ('applications', 'NNS')])]\n",
      "===> (S\n",
      "  (Chunk Banker/NNP)\n",
      "  should/MD\n",
      "  be/VB\n",
      "  able/JJ\n",
      "  to/TO\n",
      "  review/VB\n",
      "  the/DT\n",
      "  credit/NN\n",
      "  history/NN\n",
      "  and/CC\n",
      "  the/DT\n",
      "  bank/NN\n",
      "  balance/NN\n",
      "  of/IN\n",
      "  customers/NNS\n",
      "  that/WDT\n",
      "  apply/VBP\n",
      "  for/IN\n",
      "  loans/NNS\n",
      "  ./.\n",
      "  This/DT\n",
      "  will/MD\n",
      "  help/VB\n",
      "  banker/NN\n",
      "  to/TO\n",
      "  decide/VB\n",
      "  wether/JJR\n",
      "  to/TO\n",
      "  approve/VB\n",
      "  or/CC\n",
      "  reject/VB\n",
      "  the/DT\n",
      "  loan/NN\n",
      "  applications/NNS)\n",
      "Chunked Verbs:: [Tree('S', [('Banker', 'NN')]), Tree('S', [('Banker', 'NNP'), ('should', 'MD'), Tree('Chunk', [('be', 'VB')]), ('able', 'JJ'), ('to', 'TO'), Tree('Chunk', [('review', 'VB')]), ('the', 'DT'), ('credit', 'NN'), ('history', 'NN'), ('and', 'CC'), ('the', 'DT'), ('bank', 'NN'), ('balance', 'NN'), ('of', 'IN'), ('customers', 'NNS'), ('that', 'WDT'), Tree('Chunk', [('apply', 'VBP')]), ('for', 'IN'), ('loans', 'NNS'), ('.', '.'), ('This', 'DT'), ('will', 'MD'), Tree('Chunk', [('help', 'VB')]), ('banker', 'NN'), ('to', 'TO'), Tree('Chunk', [('decide', 'VB')]), ('wether', 'JJR'), ('to', 'TO'), Tree('Chunk', [('approve', 'VB')]), ('or', 'CC'), Tree('Chunk', [('reject', 'VB')]), ('the', 'DT'), ('loan', 'NN'), ('applications', 'NNS')])]\n",
      "Chunked ProNOuns:: [Tree('S', [('Banker', 'NN')]), Tree('S', [Tree('Chunk', [('Banker', 'NNP')]), ('should', 'MD'), ('be', 'VB'), ('able', 'JJ'), ('to', 'TO'), ('review', 'VB'), ('the', 'DT'), ('credit', 'NN'), ('history', 'NN'), ('and', 'CC'), ('the', 'DT'), ('bank', 'NN'), ('balance', 'NN'), ('of', 'IN'), ('customers', 'NNS'), ('that', 'WDT'), ('apply', 'VBP'), ('for', 'IN'), ('loans', 'NNS'), ('.', '.'), ('This', 'DT'), ('will', 'MD'), ('help', 'VB'), ('banker', 'NN'), ('to', 'TO'), ('decide', 'VB'), ('wether', 'JJR'), ('to', 'TO'), ('approve', 'VB'), ('or', 'CC'), ('reject', 'VB'), ('the', 'DT'), ('loan', 'NN'), ('applications', 'NNS')])]\n",
      "Filtered Sentence--> ['Banker', 'Banker', 'able', 'review', 'credit', 'history', 'bank', 'balance', 'customers', 'apply', 'loans', 'will', 'help', 'banker', 'decide', 'wether', 'approve', 'reject', 'loan', 'applications']\n"
     ]
    }
   ],
   "source": [
    "wb = open_workbook(\"MYLOCALEXCELBRD_mod.xlsx\")\n",
    "# FOr each sheet in Spreadsheet\n",
    "for sheet in wb.sheets():\n",
    "        numberRows = sheet.nrows\n",
    "        numberCols = sheet.ncols\n",
    "        \n",
    "        items = []\n",
    "        rows = []\n",
    "        # For each row in a workbook\n",
    "        for row in range(1, numberRows):\n",
    "            values =  []\n",
    "            filtered_Sentence = []\n",
    "            verbs = []\n",
    "            proNouns = []\n",
    "            # For each columns in the workbook\n",
    "            for col in range(1, numberCols):\n",
    "                value = (sheet.cell(row, col).value)\n",
    "                values.append(value)\n",
    "                # tokenize the words in the sentence\n",
    "                print(sent_tokenize(value))\n",
    "                print(\"*********************************************\")\n",
    "                words = word_tokenize(value)\n",
    "                # Words are tagged so, that they can be identified which one is verb/Noun/ Pronoun\n",
    "                tagged = nltk.pos_tag(words)\n",
    "                print (\"Tagged:\",tagged)\n",
    "                #Retrieves Verb from the Sentence or tokens\n",
    "                chunkVerb = r\"\"\"Chunk: {<VB.?>*} \"\"\"\n",
    "                chunkProNoun = r\"\"\"Chunk: {<NNP.?>*} \"\"\"\n",
    "                \n",
    "                chunkParser = nltk.RegexpParser(chunkVerb)\n",
    "                chunked = chunkParser.parse(tagged)\n",
    "                print (\"---->\",chunked)\n",
    "                verbs.append(chunked)\n",
    "                print (\"Grouped VERBS ::\", verbs)\n",
    "                chunkParser = nltk.RegexpParser(chunkProNoun)\n",
    "                chunked = chunkParser.parse(tagged)\n",
    "                print(\"===>\",chunked)\n",
    "                proNouns.append(chunked)\n",
    "                print (\"Chunked Verbs::\",verbs)\n",
    "                print (\"Chunked ProNOuns::\",proNouns)\n",
    "                \n",
    "                #chunked.draw()\n",
    "                \n",
    "               \n",
    "                for w in words:\n",
    "                    if w not in stopWords:\n",
    "                        filtered_Sentence.append(w)\n",
    "            print(\"Filtered Sentence-->\",filtered_Sentence)\n",
    "                \n",
    "\n",
    "        break;\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Utility functions for Engineering Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Process flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Prepare data **\n",
    "* Load artifacts from object storage and create pandas dataframes\n",
    "* Prepare the pandas dataframes. Add additional columns required for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirements.xlsx\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './/test/Requirements.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-ed233fa7233d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mload_artifacts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#prepare_artifact_dataframes()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-bcd74ccafd6a>\u001b[0m in \u001b[0;36mload_artifacts\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m   \u001b[0;31m# we had to use so many variables as we were not able to run python 3 with boto3 get_object()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mrequirements_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrequirements_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrequirements_sheet_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mdomain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdomain_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdomain_sheet_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mdataelements_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataelements_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataelements_sheet_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_deprecate_kwarg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/excel.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, skiprows, skip_footer, index_col, names, usecols, parse_dates, date_parser, na_values, thousands, convert_float, converters, dtype, true_values, false_values, engine, squeeze, **kwds)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     return io._parse_excel(\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/excel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, io, **kwds)\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_contents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             raise ValueError('Must explicitly set engine if not passing in'\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/xlrd/__init__.py\u001b[0m in \u001b[0;36mopen_workbook\u001b[0;34m(filename, logfile, verbosity, use_mmap, file_contents, encoding_override, formatting_info, on_demand, ragged_rows)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_contents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpeek\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb\"PK\\x03\\x04\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# a ZIP file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './/test/Requirements.xlsx'"
     ]
    }
   ],
   "source": [
    "load_artifacts()\n",
    "#prepare_artifact_dataframes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Run Spacy Text Classifier on data **\n",
    "* Add the text classification output to the artifact dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_column_name = \"ClassifiedText\"\n",
    "#defects_df = add_text_classifier_output(defects_df,config, output_column_name)\n",
    "#testcases_df = add_text_classifier_output(testcases_df,config, output_column_name)\n",
    "#requirements_df = add_text_classifier_output(requirements_df,config, output_column_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Populate keywords and entities **\n",
    "* Add the keywords and entities extracted from the unstructured text to the artifact dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Correlate keywords between artifacts **\n",
    "* Add the text similarity score of associated artifacts to the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Utility functions to store entities and relations in Orient DB **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Transform results for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Expose integration point with a websocket client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Start websocket client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_websocket_listener()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
